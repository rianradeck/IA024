{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem (Bengio 2003) - MLP + Embeddings\n",
        "\n",
        "Neste exercício iremos treinar uma rede neural similar a do Bengio 2003 para prever a próxima palavra de um texto, data as palavras anteriores como entrada. Esta tarefa é chamada de \"Modelagem da Linguagem\".\n",
        "\n",
        "Portanto, você deve implementar o modelo de linguagem inspirado no artigo do Bengio, para prever a próxima palavra usando rede com embeddings e duas camadas.\n",
        "Sugestão de alguns parâmetros:\n",
        "* context_size = 9\n",
        "* max_vocab_size = 3000\n",
        "* embedding_dim = 64\n",
        "* usar pontuação no vocabulário\n",
        "* descartar qualquer contexto ou target que não esteja no vocabulário\n",
        "* É esperado conseguir uma perplexidade da ordem de 50.\n",
        "* Procurem fazer asserts para garantir que partes do seu programa estão testadas\n",
        "\n",
        "Este enunciado não é fixo, podem mudar qualquer um dos parâmetros acima, mas procurem conseguir a perplexidade esperada ou menor.\n",
        "\n",
        "Gerem alguns frases usando um contexto inicial e depois deslocando o contexto e prevendo a próxima palavra gerando frases compridas para ver se está gerando texto plausível.\n",
        "\n",
        "Algumas dicas:\n",
        "- Inclua caracteres de pontuação (ex: `.` e `,`) no vocabulário.\n",
        "- Deixe tudo como caixa baixa (lower-case).\n",
        "- A escolha do tamanho do vocabulario é importante: ser for muito grande, fica difícil para o modelo aprender boas representações. Se for muito pequeno, o modelo apenas conseguirá gerar textos simples.\n",
        "- Remova qualquer exemplo de treino/validação/teste que tenha pelo menos um token desconhecido (ou na entrada ou na saída).\n",
        "- Durante a depuração, faça seu dataset ficar bem pequeno, para que a depuração seja mais rápida e não precise de device. Somente ligue a device quando o seu laço de treinamento já está funcionando\n",
        "- Não deixe para fazer esse exercício na véspera. Ele é trabalhoso.\n",
        "\n",
        "Procure por `TODO` para entender onde você precisa inserir o seu código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "!wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "!wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [],
      "source": [
        "# Simples limitação dos dados, para trabalhar apenas com tokens presentes no livro.\n",
        "\n",
        "text = open(\"67724.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text.find(\"PARTE\\n\\n\")\n",
        "idx2 = text.find(\"*** END OF THE PROJECT\")\n",
        "text = text[idx:idx2]\n",
        "text2 = open(\"67725.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text2.find(\"PARTE\\n\\n\")\n",
        "idx2 = text2.find(\"*** END OF THE PROJECT\")\n",
        "text2 = text2[idx:idx2]\n",
        "\n",
        "text += text2\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [],
      "source": [
        "# cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "import re\n",
        "\n",
        "cleaned_paragraphs = []\n",
        "full_text = \"\"\n",
        "final_tokens = []\n",
        "# Tratando tokens em cada prágrafo\n",
        "for paragraph in paragraphs:\n",
        "    paragraph = paragraph.replace(\"\\n\", \" \")\n",
        "    for removable in [\"«\", \"»\", \"_\"]:\n",
        "        paragraph = paragraph.replace(removable, '') # Removendo as aspas, underline, etc.\n",
        "    \n",
        "    paragraph = paragraph.lower().strip() # Caixa baixa e removendo leading e trailing spaces.\n",
        "\n",
        "    if paragraph[:3] == \"pag\":\n",
        "        continue\n",
        "    if len(paragraph) < 3:\n",
        "        continue\n",
        "\n",
        "    paragraph = re.sub(\"[ ]+\", \" \", paragraph) # Espaços duplicados\n",
        "\n",
        "    for punctuation in ['.', ',', ';', '!', \":\", \"?\", \"--\"]:\n",
        "        paragraph = paragraph.replace(punctuation, (' ' + punctuation) if punctuation != \"--\" else (punctuation + ' ')) # Tratando pontuação como próprio token\n",
        "    cleaned_paragraphs.append(paragraph)\n",
        "    final_tokens += paragraph.split(\" \") + ['\\n']\n",
        "    full_text += paragraph + '\\n'\n",
        "    \n",
        "for paragraph in cleaned_paragraphs:\n",
        "    print(paragraph)\n",
        "\n",
        "# print(final_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [],
      "source": [
        "# Conta as palavras no dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        if text == \"\\n\":\n",
        "            word_counts.update(text)\n",
        "            continue\n",
        "        # word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "        word_counts.update(list(re.findall(r'.*', text.lower())))\n",
        "        \n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(final_tokens)\n",
        "word_counts.pop('')\n",
        "\n",
        "print(word_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 2500\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    if isinstance(sentence, str):\n",
        "        sentence = sentence.split(\" \")\n",
        "    # print(sentence)\n",
        "    return [vocab.get(word, 0) for word in sentence]\n",
        "\n",
        "encode_sentence(cleaned_paragraphs[20], vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [],
      "source": [
        "context_size = 5 # 5 palavras de entrada. O target é a próxima palavra\n",
        "\"\"\"TODO: Preparar o dataset\"\"\"\n",
        "overlap_size = 4\n",
        "step = context_size - overlap_size\n",
        "if step <= 0:\n",
        "    raise\n",
        "\n",
        "# print(final_tokens)\n",
        "whole_data = []\n",
        "for i in range(0, len(final_tokens) - context_size, step):\n",
        "    cur_data = [encode_sentence(final_tokens[i:i+context_size], vocab), encode_sentence(final_tokens[i + context_size], vocab)[0]]\n",
        "    if 0 in cur_data[0] or 0 == cur_data[1]:# or vocab_size in cur_data[0] or vocab_size == cur_data[1] :\n",
        "        continue\n",
        "    for i in range(5):\n",
        "        cur_data[0][i] -= 1\n",
        "    cur_data[1] -= 1\n",
        "    whole_data.append(tuple(cur_data))\n",
        "\n",
        "print(whole_data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "outputs": [],
      "source": [
        "\"\"\"TODO: divida o dataset em validação/treino com um proporção de 20/80 %. OBS, use random_state=18\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "N = len(whole_data)\n",
        "random_state = 18\n",
        "np.random.seed(random_state)\n",
        "random_indices = np.arange(N)\n",
        "np.random.shuffle(random_indices)\n",
        "# print(random_indices)\n",
        "cut_idx = int(0.8 * N)\n",
        "train_indices = random_indices[:cut_idx]\n",
        "validation_indices = random_indices[cut_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [],
      "source": [
        "\"\"\"TODO: implemente a classe do dataset\"\"\"\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        idxs = train_indices if split == \"train\" else validation_indices\n",
        "        self.data = []\n",
        "        for idx in idxs:\n",
        "            self.data.append(whole_data[idx])\n",
        "            \n",
        "        self.vocab = vocab  # Set vocabulary\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Return the length of the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line, label = self.data[idx]  # Get label and text for specified index\n",
        "\n",
        "        return torch.tensor(line), torch.tensor(label)\n",
        "\n",
        "train_data = MyDataset(split=\"train\", vocab=vocab)\n",
        "val_data = MyDataset(split=\"val\", vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "batch_size = 30\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "sample = next(iter(train_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import multiprocessing\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, h) # This hidden layer ideia I've got from Gabriel Freita's code. It helped to reduce PPL in 20.\n",
        "        self.linear3 = nn.Linear(h, vocab_size, bias = False)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        out = self.relu(self.linear2(out))\n",
        "        out = self.linear3(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "embedding_dim = 128\n",
        "context_size = 5\n",
        "H = 500\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper function to get accuracy from log probabilities\n",
        "def get_accuracy_from_log_probs(log_probs, labels):\n",
        "    probs = torch.exp(log_probs)\n",
        "    predicted_label = torch.argmax(probs, dim=1)\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "# helper function to evaluate model on dev data\n",
        "def evaluate(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        dev_st = time.time()\n",
        "        for it, data_tensor in enumerate(dataloader):\n",
        "            input = data_tensor[:,0:2]\n",
        "            target = data_tensor[:,2]\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            log_probs = model(input)\n",
        "            mean_loss += criterion(log_probs, target).item()\n",
        "            mean_acc += get_accuracy_from_log_probs(log_probs, target)\n",
        "            count += 1\n",
        "            if it % 500 == 0: \n",
        "                print(f\"Dev Iteration {it} complete. Mean Loss: {mean_loss / count}; Mean Acc: {mean_acc / count}; Time taken (s): {time.time()-dev_st}\")\n",
        "                dev_st = time.time()\n",
        "\n",
        "    return mean_acc / count, mean_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica se há uma device disponível e define o dispositivo para device se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "print(input.shape, target.shape)\n",
        "output = model(input)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training and Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [],
      "source": [
        "# Verifica se há uma device disponível e define o dispositivo para device se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper function to get accuracy from log probabilities\n",
        "def get_accuracy_from_log_probs(log_probs, labels):\n",
        "    probs = torch.exp(log_probs)\n",
        "    predicted_label = torch.argmax(probs, dim=1)\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "# helper function to evaluate model on dev data\n",
        "def evaluate(model, criterion, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_tensor, target_tensor in dataloader:\n",
        "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "            log_probs = model(context_tensor)\n",
        "            mean_loss += criterion(log_probs, target_tensor).item()\n",
        "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [],
      "source": [
        "# Using negative log-likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "# create model\n",
        "model = LanguageModel(len(vocab), embedding_dim, context_size, H)\n",
        "\n",
        "# load it to gpu\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "\n",
        "train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "print(\"\\n--- Evaluating model on train data ---\")\n",
        "print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "\n",
        "best_test_ppl = 1e9\n",
        "for epoch in range(10):\n",
        "    st = time.time()\n",
        "    print(f\"\\n--- Training model Epoch: {epoch+1} ---\")\n",
        "    for it, data_tensor in enumerate(train_loader):       \n",
        "        context_tensor = data_tensor[0]\n",
        "        target_tensor = data_tensor[1]\n",
        "\n",
        "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "\n",
        "        # zero out the gradients from the old instance\n",
        "        model.zero_grad()\n",
        "        # get log probabilities over next words\n",
        "        log_probs = model(context_tensor)\n",
        "        # compute loss function\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "        # backward pass and update gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Finished training of Epoch {epoch +1}\\n--- Evaluating model on train data ---\")\n",
        "    train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "    print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "    print(\"\\n--- Evaluating model on test data ---\")\n",
        "    test_acc, test_loss = evaluate(model, loss_function, val_loader)\n",
        "    print(f\"Test Accuracy: {test_acc}; Test Loss: {test_loss}, Test PPL: {torch.exp(torch.tensor(test_loss))}\")\n",
        "\n",
        "    best_test_ppl = min(best_test_ppl, (torch.exp(torch.tensor(test_loss))))\n",
        "\n",
        "print(\"BEST PPL:\", best_test_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [],
      "source": [
        "i = 1000\n",
        "text = \" \".join(final_tokens[i: i+context_size])\n",
        "\n",
        "inv_vocab = {v-1 : k for k, v in vocab.items()}\n",
        "def generate_text(model, vocab, text, max_length, context_size):\n",
        "    context = encode_sentence(text, vocab)\n",
        "\n",
        "    final_text = context\n",
        "    for i in range(max_length):\n",
        "        inputs = torch.tensor(context).to(device).view((1, -1))\n",
        "        pred = torch.argmax(model(inputs), dim=1)\n",
        "        final_text.append(pred.item())\n",
        "        context = final_text[-context_size:]\n",
        "\n",
        "    l = ([inv_vocab[t] for t in final_text])\n",
        "    decoded_sentence = \" \".join(l)\n",
        "\n",
        "    print(decoded_sentence)\n",
        "\n",
        "\n",
        "context = context_size\n",
        "max_length= 100\n",
        "generate_text(model, vocab, text, max_length, context_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
