{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 10_11 - Implementação do RAGAS\n",
    "\n",
    "- Implementar o RAGAS com o LLaMA-3 70B para avaliar a qualidade das 50 anotações do IIRC usadas no exercício passado.\n",
    "- O RAGAS considera context, question, answer, keys que estão disponíveis no conjunto de teste do IIRC.\n",
    "\n",
    "Opcional:\n",
    "- Avaliar as respostas do exercício da aula 9_10\n",
    "- Usar multi agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Unicamp\\IA024\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os\n",
    "import sentence_transformers\n",
    "import numpy as np\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "from functools import reduce\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Original RAGAs prompts\n",
    "from ragas.metrics._faithfulness import LONG_FORM_ANSWER_PROMPT, NLI_STATEMENTS_MESSAGE\n",
    "from ragas.metrics._answer_relevance import QUESTION_GEN\n",
    "from ragas.metrics._context_relevancy import CONTEXT_RELEVANCE\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions_file = open(\"./test_questions.json\", \"r\")\n",
    "test_questions = json.load(test_questions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Zeus know for in Greek mythology?',\n",
       " 'answer': 'sky and thunder god',\n",
       " 'context': 'he Palici the sons of Zeus. in Greek mythology. Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion. ',\n",
       " 'n_context': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = {}\n",
    "\n",
    "def process_answer(answer):\n",
    "    match answer[\"type\"]:\n",
    "        case \"span\":\n",
    "            return answer[\"answer_spans\"][0][\"text\"]\n",
    "        case \"value\":\n",
    "            return answer[\"answer_value\"] + \" \" + answer[\"answer_unit\"]\n",
    "        case \"binary\":\n",
    "            return answer[\"answer_value\"]\n",
    "        case _:\n",
    "            print(\"Unsupported type\", answer[\"type\"])\n",
    "\n",
    "def process_context(context):\n",
    "    merged_context = \"\"\n",
    "    for _context in context:\n",
    "        merged_context += _context[\"text\"] + '. '\n",
    "    return merged_context\n",
    "\n",
    "questions_list = []\n",
    "for test_question in test_questions:\n",
    "    question = test_question[\"question\"]\n",
    "    answer = process_answer(test_question[\"answer\"])\n",
    "    context = process_context(test_question[\"context\"])\n",
    "    questions_list.append(\n",
    "        {\n",
    "            \"question\" : question,\n",
    "            \"answer\" : answer,\n",
    "            \"context\": context,\n",
    "            \"n_context\": len(test_question[\"context\"])\n",
    "        }\n",
    "    )\n",
    "\n",
    "questions_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Você está falando português!\\n\\n\"Opa\" é uma expressão comum em português, especialmente no Brasil, que pode ser usada para expressar surpresa, entusiasmo ou alegria. Já \"beleza\" significa \"beleza\" ou \"lindo\".\\n\\nEntão, se você está perguntando \"Opa, beleza?\", você está perguntando se tudo está bem ou se algo é lindo.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GroqAPI():\n",
    "    def __init__(self, prompt=None, json_format=False, temperature=0):\n",
    "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY1\"))\n",
    "        \n",
    "        self.model = \"llama3-70b-8192\"\n",
    "        self.prompt = prompt\n",
    "        self.json_format = json_format\n",
    "        self.temperature = temperature\n",
    "\n",
    "    \n",
    "    def get_answer(self, prompt : str):\n",
    "        messages = []\n",
    "\n",
    "        if self.prompt:\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\" : \"system\",\n",
    "                    \"content\" : self.prompt.instruction\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if self.prompt.examples:\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\" : \"system\",\n",
    "                        \"content\" : f\"You MUST output in JSON exactly like this example:\\n{self.prompt.examples[0]}\\n\"\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\" : \"system\",\n",
    "                    \"content\" : f\"You will receive a JSON with the following keys: {self.prompt.input_keys}. You must include the received keys and this additional key {self.prompt.output_key} in the output, just like the given example.\"\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : prompt\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        answer = self.client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            response_format=({\"type\": \"json_object\"} if self.json_format else None)\n",
    "        )\n",
    "\n",
    "        return answer.choices[0].message.content\n",
    "    \n",
    "api = GroqAPI()\n",
    "api.get_answer(\"Opa, beleza?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faithfulness():\n",
    "    def __init__(self, json=True) -> None:\n",
    "        self.statement_agent = GroqAPI(\n",
    "            prompt=LONG_FORM_ANSWER_PROMPT,\n",
    "            json_format=json\n",
    "        )\n",
    "\n",
    "        self.verdict_agent = GroqAPI(\n",
    "            prompt=NLI_STATEMENTS_MESSAGE,\n",
    "            json_format=json\n",
    "        )\n",
    "\n",
    "    def make_sentences(self, answer):\n",
    "        d = {idx : sentence for idx, sentence in enumerate(answer.split(\".\"))}\n",
    "        s = \"\\\\n        \"\n",
    "        for k, v in d.items():\n",
    "            s += f\"{k}:{v}\\\\n        \"\n",
    "        return s\n",
    "\n",
    "    def get_statements(self, question, answer, simpler_format=True, debug=False):\n",
    "        prompt = f\"\"\"\\u007b\n",
    "  \"question\": \"{question}\",\n",
    "  \"answer\": \"{answer}\",\n",
    "  \"sentences\": \"{self.make_sentences(answer)}\"\n",
    "\\u007d\n",
    "\"\"\"\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Getting simpler sentences from the answer and question:\\n{prompt}\")\n",
    "\n",
    "        agent_answer = json.loads(self.statement_agent.get_answer(prompt))\n",
    "        if simpler_format:\n",
    "            agent_answer = reduce(lambda x,y :x+y , [analyse[\"simpler_statements\"] for analyse in agent_answer[\"analysis\"]])\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Simpler sentences: {json.dumps(agent_answer, indent=2)}\")\n",
    "\n",
    "        return agent_answer\n",
    "\n",
    "    def get_verdicts(self, statements, context, debug=False):\n",
    "        statements = \"\\\"\\n    \\\"\".join(statements)\n",
    "        prompt = f\"\"\"\\u007b\n",
    "  \"context\": \"{context}\",\n",
    "  \"statements\": [\n",
    "    \"{statements}\"\n",
    "  ]\n",
    "\\u007d\n",
    "\"\"\"\n",
    "        if debug:\n",
    "            print(f\"We're comparing statements (generated from the original answer) to the context\\nBy doing so we can see if a given part of the answer (statement) was actually given from the context\\n{prompt}\")\n",
    "\n",
    "        verdict = json.loads(self.verdict_agent.get_answer(prompt))\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Verdicts: {json.dumps(verdict, indent=2)}\")\n",
    "\n",
    "        return verdict\n",
    "\n",
    "    def calculate_faithfulness(self, sample, debug=False):\n",
    "        statements = self.get_statements(sample[\"question\"], sample[\"answer\"], simpler_format=True, debug=debug)\n",
    "        verdicts = self.get_verdicts(statements, sample[\"context\"], debug=debug)\n",
    "        supported = 0\n",
    "        for verdict in verdicts[\"answer\"]:\n",
    "            supported += verdict[\"verdict\"]\n",
    "        return supported / len(verdicts[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Unicamp\\IA024\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35710054636001587"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AnswerRelevance():\n",
    "    def __init__(self, json=True):\n",
    "        self.question_generator_agent = GroqAPI(\n",
    "            prompt=QUESTION_GEN,\n",
    "            json_format=json,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        self.embedding_model = sentence_transformers.SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "    \n",
    "    def generate_questions(self, answer, context, n=3):\n",
    "        prompt = f\"\"\"\\u007b\n",
    "  \"answer\": \"{answer}\",\n",
    "  \"context\": \"{context}\"\n",
    "\\u007d\n",
    "\"\"\"\n",
    "        questions = []\n",
    "        for _ in range(n):\n",
    "            questions.append(json.loads(self.question_generator_agent.get_answer(prompt)))\n",
    "\n",
    "        return questions\n",
    "    \n",
    "    def calculate_embedding(self, text):\n",
    "        return self.embedding_model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "    def calculate_relevance(self, sample):\n",
    "        actual_question = sample[\"question\"]\n",
    "        actual_question_emb = self.calculate_embedding(actual_question)\n",
    "\n",
    "        generated_questions = self.generate_questions(sample[\"answer\"], sample[\"context\"], n=1)\n",
    "        generated_questions_emb = []\n",
    "\n",
    "        for question in generated_questions:\n",
    "            question = question[\"output\"][\"question\"]\n",
    "            generated_questions_emb.append(self.calculate_embedding(question))\n",
    "\n",
    "        def cosine_sim(a, b):\n",
    "            return dot(a, b)/(norm(a)*norm(b))\n",
    "        \n",
    "        relevance = 0\n",
    "        for emb in generated_questions_emb:\n",
    "            relevance += cosine_sim(emb, actual_question_emb)\n",
    "        relevance /= len(generated_questions_emb)\n",
    "        \n",
    "        return relevance\n",
    "\n",
    "\n",
    "answer_relevance = AnswerRelevance(json=True)\n",
    "answer_relevance.calculate_relevance(questions_list[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ContextRelevance():\n",
    "    def __init__(self, json=True):\n",
    "        self.context_relevancy_agent = GroqAPI(\n",
    "            prompt=CONTEXT_RELEVANCE,\n",
    "            json_format=json,\n",
    "            temperature=0\n",
    "        )\n",
    "    \n",
    "    def get_relevant_sentences(self, question, context):\n",
    "        prompt=f\"\"\"\\u007b\n",
    "  \"question\": \"{question}\",\n",
    "  \"context\": \"{context}\"\n",
    "\\u007d\n",
    "\"\"\"\n",
    "        return json.loads(self.context_relevancy_agent.get_answer(prompt))\n",
    "\n",
    "    def calculate_relevance(self, sample):\n",
    "        extracted_sentences = self.get_relevant_sentences(sample[\"question\"], sample[\"context\"])[\"candidate sentences\"]\n",
    "        if isinstance(extracted_sentences, str): # Insufficient information\n",
    "            return 0\n",
    "        return len(extracted_sentences) / sample[\"n_context\"]\n",
    "\n",
    "ctx = ContextRelevance()\n",
    "ctx.calculate_relevance(questions_list[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_faithfulness(json):\n",
    "    faithfulness_agent = Faithfulness(json=json)\n",
    "\n",
    "    faithfulness_list = []\n",
    "    errors = []\n",
    "\n",
    "    for idx, sample in tqdm(enumerate(questions_list)):\n",
    "        try:\n",
    "            faithfulness_list.append({\"idx\": idx, \"faithfulness\": faithfulness_agent.calculate_faithfulness(sample, debug=False)})\n",
    "        except:\n",
    "            errors.append(idx)\n",
    "    \n",
    "    return faithfulness_list, errors\n",
    "\n",
    "def print_faithfulness_infos(faithfulness_list, errors):\n",
    "    l = np.array([x[\"faithfulness\"] for x in faithfulness_list])\n",
    "    print(l.mean(), \"+-\", l.std())\n",
    "    print(\"#Errors:\", len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [18:05, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of faithfulness evaluation with JSON casting\n",
      "0.7948717948717948 +- 0.38759529379053775\n",
      "#Errors: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "faithfulness_list_with_json, errors_with_json = eval_faithfulness(True)\n",
    "\n",
    "print(\"Result of faithfulness evaluation with JSON casting\")\n",
    "print_faithfulness_infos(faithfulness_list_with_json, errors_with_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [19:26, 23.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of faithfulness evaluation without JSON casting\n",
      "0.8333333333333334 +- 0.3639761427327792\n",
      "#Errors: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "faithfulness_list_without_json, errors_without_json = eval_faithfulness(False)\n",
    "\n",
    "print(\"Result of faithfulness evaluation without JSON casting\")\n",
    "print_faithfulness_infos(faithfulness_list_without_json, errors_without_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_answer_relevance(json):\n",
    "    answer_relevances = []\n",
    "    answer_relevance_errors = []\n",
    "    answer_relevance = AnswerRelevance(json=json)\n",
    "\n",
    "    for idx, sample in tqdm(enumerate(questions_list)):\n",
    "        try:\n",
    "            answer_relevances.append({\"idx\": idx, \"relevance\": answer_relevance.calculate_relevance(sample)})\n",
    "        except:\n",
    "            answer_relevance_errors.append(idx)\n",
    "    \n",
    "    return answer_relevances, answer_relevance_errors\n",
    "\n",
    "def print_answer_relevance_infos(answer_relevances, answer_relevance_errors):\n",
    "    x = np.array([relevance[\"relevance\"] for relevance in answer_relevances])\n",
    "    print(x.mean(), \"+-\", x.std())\n",
    "    print(\"#Errors:\", len(answer_relevance_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Unicamp\\IA024\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "50it [06:58,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Answer Relevance With JSON\n",
      "0.5111142351876857 +- 0.1648589007466922\n",
      "#Errors: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer_relevances_with_json, answer_relevance_errors_with_json = eval_answer_relevance(True)\n",
    "\n",
    "print(\"Evaluating Answer Relevance With JSON\")\n",
    "print_answer_relevance_infos(answer_relevances_with_json, answer_relevance_errors_with_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [07:11,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Answer Relevance Without JSON\n",
      "0.4712211094223536 +- 0.16171204232238717\n",
      "#Errors: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer_relevances_without_json, answer_relevance_errors_without_json = eval_answer_relevance(False)\n",
    "\n",
    "print(\"Evaluating Answer Relevance Without JSON\")\n",
    "print_answer_relevance_infos(answer_relevances_without_json, answer_relevance_errors_without_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_context_relevance(json):\n",
    "    context_relevances = []\n",
    "    context_relevance_errors = []\n",
    "    ctx = ContextRelevance(json=json)\n",
    "\n",
    "    for idx, sample in tqdm(enumerate(questions_list)):\n",
    "        try:\n",
    "            context_relevances.append({\"idx\": idx, \"relevance\": ctx.calculate_relevance(sample)})\n",
    "        except:\n",
    "            context_relevance_errors.append(idx)\n",
    "    \n",
    "    return context_relevances, context_relevance_errors\n",
    "\n",
    "def print_context_relevance_infos(context_relevances, context_relevance_errors):\n",
    "    x = np.array([relevance[\"relevance\"] for relevance in context_relevances])\n",
    "    print(x.mean(), \"+-\", x.std())\n",
    "    print(\"#Errors:\", len(context_relevance_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [05:53,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Context Relevance With JSON\n",
      "0.5755555555555555 +- 0.41321682945899985\n",
      "#Errors: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_relevances_with_json, context_relevance_errors_with_json = eval_context_relevance(True)\n",
    "\n",
    "print(\"Evaluating Context Relevance With JSON\")\n",
    "print_context_relevance_infos(context_relevances_with_json, context_relevance_errors_with_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [06:05,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Context Relevance Without JSON\n",
      "0.5100775193798449 +- 0.22259740877612777\n",
      "#Errors: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "context_relevances_without_json, context_relevance_errors_without_json = eval_context_relevance(False)\n",
    "\n",
    "print(\"Evaluating Context Relevance Without JSON\")\n",
    "print_context_relevance_infos(context_relevances_without_json, context_relevance_errors_without_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
