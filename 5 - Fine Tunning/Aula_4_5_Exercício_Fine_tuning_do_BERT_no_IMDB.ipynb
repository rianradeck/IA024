{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk"
      },
      "source": [
        "# Notebook de referência\n",
        "\n",
        "Nome:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ80hHaftwUd"
      },
      "source": [
        "## Instruções:\n",
        "\n",
        "\n",
        "Treinar e medir a acurácia de um modelo BERT (ou variantes) para classificação binária usando o dataset do IMDB (20k/5k amostras de treino/validação).\n",
        "\n",
        "Importante:\n",
        "- Deve-se implementar o próprio laço de treinamento.\n",
        "- Implementar o acumulo de gradiente.\n",
        "\n",
        "Dicas:\n",
        "- BERT geralmente costuma aprender bem uma tarefa com poucas épocas (de 3 a 5 épocas). Se tiver demorando mais de 5 épocas para chegar em 80% de acurácia, ajuste os hiperparametros.\n",
        "\n",
        "- Solução para erro de memória:\n",
        "  - Usar bfloat16 permite quase dobrar o batch size\n",
        "\n",
        "Opcional:\n",
        "- Pode-se usar a função trainer da biblioteca Transformers/HuggingFace para verificar se seu laço de treinamento está correto. Note que ainda assim é obrigatório implementar o laço próprio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhpAkifICdJo"
      },
      "source": [
        "# Fixando a seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1ozXD-xYCcrT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHeZ9nAOEB0U",
        "outputId": "bdd4a1f7-e1d0-4377-9638-a4ee1e968a38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x1c526a5ebf0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wbnfzst5O3k",
        "outputId": "bebda5c0-5614-4cd0-a2f4-5754cdb9c336"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(\"aclImdb.tgz\"):\n",
        "    !curl -LO http://files.fast.ai/data/aclImdb.tgz\n",
        "    !tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (20k exemplos) e validação (5k exemplos) artificialmente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HIN_xLI_TuT",
        "outputId": "787fc595-88b1-486a-8c0c-bcde36396793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "False POSSIBLE SPOILERS<br /><br />The Spy Who Shagged Me is a muchly overrated and over-hyped sequel. Int\n",
            "False The long list of \"big\" names in this flick (including the ubiquitous John Mills) didn't bowl me over\n",
            "True Bette Midler showcases her talents and beauty in \"Diva Las Vegas\". I am thrilled that I taped it and\n",
            "3 últimas amostras treino:\n",
            "False I was previously unaware that in the early 1990's Devry University (or was it ITT Tech?) added Film \n",
            "True The story and music (George Gershwin!) are wonderful, as are Levant, Guetary, Foch, and, of course, \n",
            "True This is my favorite show. I think it is utterly brilliant. Thanks to David Chase for bringing this i\n",
            "3 primeiras amostras validação:\n",
            "True Why has this not been released? I kind of thought it must be a bit rubbish since it hasn't been. How\n",
            "True I was amazingly impressed by this movie. It contained fundamental elements of depression, grief, lon\n",
            "True photography was too jumpy to follow. dark scenes hard to see.<br /><br />Had good story line too bad\n",
            "3 últimas amostras validação:\n",
            "True In the early to mid 1970's, Clifford Irving proposed to write the ultimate biography of Howard Hughe\n",
            "True An ultra-modern house in an affluent neighborhood appears to be the cause of each of its inhabitants\n",
            "True Some of the best movies that are categorized as \"comedies\" actually blur between comedy and drama. \"\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path), encoding=\"utf-8\") as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('./aclImdb/train/pos')\n",
        "x_train_neg = load_texts('./aclImdb/train/neg')\n",
        "x_test_pos = load_texts('./aclImdb/test/pos')\n",
        "x_test_neg = load_texts('./aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start of sequence token: 101, End of sequence token: 102\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "tokens = tokenizer(x_train[0], add_special_tokens=True, padding=True, truncation=True)\n",
        "\n",
        "# print(len(tokens[\"input_ids\"]), len(x_train[0].split()))\n",
        "CLS, SEP = int(tokens[\"input_ids\"][0]), int(tokens[\"input_ids\"][-1])\n",
        "print(f\"Start of sequence token: {CLS}, End of sequence token: {SEP}\")\n",
        "len(tokens[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset / Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class IMDB(Dataset):\n",
        "    def __init__(self, X, Y, tokenizer):\n",
        "        super().__init__()\n",
        "\n",
        "        # Tokenize input\n",
        "        self.tokenized_data = tokenizer(X, add_special_tokens=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Input and Label\n",
        "        self.X = self.tokenized_data[\"input_ids\"]\n",
        "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
        "\n",
        "        # Attention Mask of the Input\n",
        "        self.mask = self.tokenized_data[\"attention_mask\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert len(self.X[idx]) == 512, \"Bad Input\"\n",
        "        assert len(self.mask[idx]) == 512, \"Bad Mask\"\n",
        "        \n",
        "        return self.X[idx], self.mask[idx], self.Y[idx]\n",
        "\n",
        "train_dataset = IMDB(x_train, y_train, tokenizer)\n",
        "valid_dataset = IMDB(x_valid, y_valid, tokenizer)\n",
        "test_dataset = IMDB(x_test, y_test, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XalnCbBX5saL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids shape torch.Size([32, 512]), attention mask shape torch.Size([32, 512])\n",
            "sample_output shape torch.Size([32, 1]), sample_label shape torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertModel\n",
        "\n",
        "class BinaryClassifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert_model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.linear = torch.nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        output = self.bert_model(input_ids=X, attention_mask=mask)\n",
        "        C = output.last_hidden_state[:, 0]\n",
        "\n",
        "        y = self.dropout(C)\n",
        "        y = self.linear(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "model = BinaryClassifier()\n",
        "\n",
        "sample_X, sample_mask, sample_label = next(iter(train_dataloader))\n",
        "print(f\"input_ids shape {sample_X.shape}, attention mask shape {sample_mask.shape}\")\n",
        "sample_output = model(sample_X, sample_mask)\n",
        "print(f\"sample_output shape {sample_output.shape}, sample_label shape {sample_label.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Treino"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funções Auxiliares do Elton\n",
        "\n",
        "Peguei as funções auxíliares do Elton pois achei bem bonito a maneira que ele mostra as informações.\n",
        "Ele também coda bem melhor que eu, se comprar nossas maneiras de computar a loss o código dele está bem mais organizado e documentado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------ Start of Elton's code ------------------------ #\n",
        "from typing import Tuple, Optional\n",
        "import tqdm # The use of TQDM makes training way more user friendly.\n",
        "\n",
        "def ppl(loss:torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the perplexity from the loss.\n",
        "\n",
        "    Args:\n",
        "        loss (torch.Tensor): loss to compute the perplexity.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: corresponding perplexity.\n",
        "    \"\"\"\n",
        "    return torch.exp(loss)\n",
        "\n",
        "def print_info(loss_value:torch.Tensor, epoch:int, total_epochs:int, \n",
        "               time:float=0.0, accuracy:Optional[float]=None):\n",
        "    \"\"\"\n",
        "    Prints the information of a epoch.\n",
        "\n",
        "    Args:\n",
        "        loss_value (torch.Tensor): epoch loss.\n",
        "        epoch (int): epoch number.\n",
        "        total_epochs (int): total number of epochs. \n",
        "        time (float, optional): time to run the epoch. Don't print if is 0.0. Defaults to 0.0.\n",
        "        accuracy (float, optional): epoch accuracy.\n",
        "    \"\"\"\n",
        "    ppl_value = ppl(loss_value)\n",
        "\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{total_epochs}], \\\n",
        "            Loss: {loss_value.item():.4f}, \\\n",
        "            Perplexity: {ppl_value.item():.4f}', end=\"\")\n",
        "    \n",
        "    if accuracy is not None:\n",
        "        print(f', Accuracy: {100*accuracy:.4f}%')\n",
        "\n",
        "    if time != 0:\n",
        "        print(f\", Elapsed Time: {time:.2f} sec\")    \n",
        "    else:\n",
        "        print(\"\")\n",
        "\n",
        "MODE_TRAIN = 0\n",
        "MODE_EVALUATE = 1\n",
        "\n",
        "def compute_loss(model:torch.nn.Module, loader:DataLoader, \n",
        "                 criterion:torch.nn.Module, mode:int = MODE_EVALUATE, \n",
        "                 optimizer:Optional[torch.optim.Optimizer]=None, \n",
        "                 accumulation_steps:Optional[int] = 1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Computes the loss from a model across a dataset.\n",
        "\n",
        "    If in train mode also runs optimizer steps.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): model to evaluate.\n",
        "        loader (DataLoader): dataset.\n",
        "        criterion (torch.nn.Module): loss function to compute.\n",
        "        mode (int): mode of the computation. \n",
        "                    If MODE_EVALUATE, computes without gradient, in eval mode and detachs loss.\n",
        "                    If MODE_TRAIN, computes with gradient and in train mode.\n",
        "                    Default is MODE_EVALUATE.\n",
        "        optimizer (torch.optim.Optimizer, optional): optimizer to use in the train mode.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: resulting loss.\n",
        "        torch.Tensor: resulting accuracy\n",
        "    \"\"\"\n",
        "    device = next(iter(model.parameters())).device\n",
        "\n",
        "    if mode == MODE_EVALUATE:\n",
        "        model.eval()\n",
        "        torch.set_grad_enabled(False)\n",
        "    elif mode == MODE_TRAIN:\n",
        "        model.train()\n",
        "        torch.set_grad_enabled(True)\n",
        "        optimizer.zero_grad()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mode: {mode}.\")\n",
        "\n",
        "    batch_index = 0\n",
        "    total_loss = torch.tensor(0, dtype=torch.float32, device=device)\n",
        "    correct = torch.tensor(0, dtype=torch.float32, device=device)\n",
        "    n = 0\n",
        "    for inputs, masks, targets in tqdm.tqdm(loader):\n",
        "        inputs = inputs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        targets = targets.reshape(-1)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        logits = model(inputs, masks)\n",
        "        logits = logits.view(-1, logits.shape[-1])\n",
        "\n",
        "        loss : torch.Tensor = criterion(logits.squeeze(), targets)\n",
        "        total_loss += loss*targets.size(0)\n",
        "        \n",
        "        predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "        n += targets.size(0)\n",
        "\n",
        "        if mode == MODE_TRAIN:\n",
        "            loss /= accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if ((batch_index+1) % accumulation_steps == 0) or (batch_index+1 == len(loader)):\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        batch_index += 1\n",
        "\n",
        "    total_loss /= n \n",
        "    accuracy = correct / n\n",
        "    \n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    accuracy = accuracy.detach()\n",
        "    total_loss = total_loss.detach()\n",
        "\n",
        "    return total_loss, accuracy\n",
        "\n",
        "# ------------------------ End of Elton's code ------------------------ #\n",
        "# Thanks Elton :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Laço de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 3060 Ti\n"
          ]
        }
      ],
      "source": [
        "# What device are we using?\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 3/157 [00:44<37:59, 14.80s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [20], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# First Epoch\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m first_loss, first_accuracy \u001b[38;5;241m=\u001b[39m compute_loss(model, valid_dataloader, criterion, MODE_EVALUATE)\n\u001b[0;32m     16\u001b[0m print_info(loss_value\u001b[38;5;241m=\u001b[39mfirst_loss, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, total_epochs\u001b[38;5;241m=\u001b[39mn_epoch, accuracy\u001b[38;5;241m=\u001b[39mfirst_accuracy)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epoch):\n",
            "Cell \u001b[1;32mIn [15], line 100\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, loader, criterion, mode, optimizer, accumulation_steps)\u001b[0m\n\u001b[0;32m     97\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m*\u001b[39mtargets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     99\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(logits\u001b[38;5;241m.\u001b[39msqueeze()))\n\u001b[1;32m--> 100\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m MODE_TRAIN:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "from time import strftime, localtime\n",
        "\n",
        "accumulation_steps = 8\n",
        "lr = 2e-5\n",
        "n_epoch = 3\n",
        "\n",
        "model = BinaryClassifier()\n",
        "model.to(device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# First Epoch\n",
        "first_loss, first_accuracy = compute_loss(model, valid_dataloader, criterion, MODE_EVALUATE)\n",
        "print_info(loss_value=first_loss, epoch=-1, total_epochs=n_epoch, accuracy=first_accuracy)\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    start = time.time() \n",
        "\n",
        "    loss_train, accuracy_train = compute_loss(model, train_dataloader, criterion, MODE_TRAIN, optimizer, accumulation_steps)\n",
        "    ppl_train = ppl(loss_train)\n",
        "\n",
        "    print_info(loss_train, epoch, n_epoch, time.time() - start, accuracy_train)\n",
        "\n",
        "    loss_val, accuracy_val = compute_loss(model, valid_dataloader, criterion, MODE_EVALUATE)\n",
        "    ppl_val = ppl(loss_val)\n",
        "    \n",
        "    print(\"VALIDATION INFO\", end=\" \")\n",
        "    print_info(loss_val, epoch, n_epoch, accuracy=accuracy_val)\n",
        "\n",
        "model_name = f\"BINARY_CLASSIFIER_BERT_{strftime('%Y-%m-%d_%H-%M-%S', localtime())}\"\n",
        "torch.save(model, model_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
