{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção\n",
        "\n",
        "Este exercício é similar ao da aula passada, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada.\n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 34):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente, mas fácil de entender) e outra matricial (eficiente mas difícil de entender). Usar slide 36 como referência.\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'wget' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "!wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import multiprocessing\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch import nn, Tensor\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4816"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simples limitação dos dados, para trabalhar apenas com tokens presentes no livro.\n",
        "\n",
        "text = open(\"67724.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text.find(\"PARTE\\n\\n\")\n",
        "idx2 = text.find(\"*** END OF THE PROJECT\")\n",
        "text = text[idx:idx2]\n",
        "text2 = open(\"67725.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text2.find(\"PARTE\\n\\n\")\n",
        "idx2 = text2.find(\"*** END OF THE PROJECT\")\n",
        "text2 = text2[idx:idx2]\n",
        "\n",
        "text += text2\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [],
      "source": [
        "# cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "cleaned_paragraphs = []\n",
        "full_text = \"\"\n",
        "final_tokens = []\n",
        "# Tratando tokens em cada prágrafo\n",
        "for paragraph in paragraphs:\n",
        "    paragraph = paragraph.replace(\"\\n\", \" \")\n",
        "    for removable in [\"«\", \"»\", \"_\"]:\n",
        "        paragraph = paragraph.replace(removable, '') # Removendo as aspas, underline, etc.\n",
        "    \n",
        "    paragraph = paragraph.lower().strip() # Caixa baixa e removendo leading e trailing spaces.\n",
        "\n",
        "    if paragraph[:3] == \"pag\":\n",
        "        continue\n",
        "    if len(paragraph) < 3:\n",
        "        continue\n",
        "\n",
        "    paragraph = re.sub(\"[ ]+\", \" \", paragraph) # Espaços duplicados\n",
        "\n",
        "    for punctuation in ['.', ',', ';', '!', \":\", \"?\", \"--\"]:\n",
        "        paragraph = paragraph.replace(punctuation, (' ' + punctuation) if punctuation != \"--\" else (punctuation + ' ')) # Tratando pontuação como próprio token\n",
        "    cleaned_paragraphs.append(paragraph)\n",
        "    final_tokens += paragraph.split(\" \") + ['\\n']\n",
        "    full_text += paragraph + '\\n'\n",
        "    \n",
        "# for paragraph in cleaned_paragraphs:\n",
        "#     print(paragraph)\n",
        "\n",
        "# final_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "127721"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        if text == \"\\n\":\n",
        "            word_counts.update(text)\n",
        "            continue\n",
        "        # word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "        word_counts.update(list(re.findall(r'.*', text.lower())))\n",
        "        \n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(final_tokens)\n",
        "word_counts.pop('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 2500\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1360, 2386, 50, 886, 1243, 1, 1536, 225, 0, 1, 11, 0, 7, 0, 1, 11, 1120, 879, 1, 0, 11, 103, 8, 1366, 14, 335, 1357, 86, 104, 4, 91, 12, 82, 35, 0, 26, 0, 593, 18, 14, 1362, 8, 580, 945, 2]\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    if isinstance(sentence, str):\n",
        "        sentence = sentence.split(\" \")\n",
        "    # print(sentence)\n",
        "    return [vocab.get(word, 0) for word in sentence]\n",
        "\n",
        "print(encode_sentence(cleaned_paragraphs[20], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[([1, 2, 35, 5, 591], 36), ([2, 35, 5, 591, 36], 1355), ([35, 5, 591, 36, 1355], 6), ([5, 591, 36, 1355, 6], 1356), ([591, 36, 1355, 6, 1356], 23)]\n"
          ]
        }
      ],
      "source": [
        "context_size = 5\n",
        "\"\"\"TODO: Preparar o dataset\"\"\"\n",
        "overlap_size = 4\n",
        "step = context_size - overlap_size\n",
        "if step <= 0:\n",
        "    raise\n",
        "\n",
        "# print(final_tokens)\n",
        "whole_data = []\n",
        "for i in range(0, len(final_tokens) - context_size, step):\n",
        "    cur_data = [encode_sentence(final_tokens[i:i+context_size], vocab), encode_sentence(final_tokens[i + context_size], vocab)[0]]\n",
        "    if 0 in cur_data[0] or 0 == cur_data[1]:# or vocab_size in cur_data[0] or vocab_size == cur_data[1] :\n",
        "        continue\n",
        "    for i in range(context_size):\n",
        "        cur_data[0][i] -= 1\n",
        "    cur_data[1] -= 1\n",
        "    whole_data.append(tuple(cur_data))\n",
        "\n",
        "print(whole_data[:context_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "outputs": [],
      "source": [
        "N = len(whole_data)\n",
        "random_state = 18\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "random_indices = np.arange(N)\n",
        "np.random.shuffle(random_indices)\n",
        "# print(random_indices)\n",
        "cut_idx = int(0.8 * N)\n",
        "train_indices = random_indices[:cut_idx]\n",
        "validation_indices = random_indices[cut_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        idxs = train_indices if split == \"train\" else validation_indices\n",
        "        self.data = []\n",
        "        for idx in idxs:\n",
        "            self.data.append(whole_data[idx])\n",
        "            \n",
        "        self.vocab = vocab  # Set vocabulary\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Return the length of the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line, label = self.data[idx]  # Get label and text for specified index\n",
        "\n",
        "        return torch.tensor(line), torch.tensor(label)\n",
        "\n",
        "train_data = MyDataset(split=\"train\", vocab=vocab)\n",
        "val_data = MyDataset(split=\"val\", vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "sample = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\rianr\\AppData\\Local\\Temp\\ipykernel_29944\\842348001.py:40: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3618.)\n",
            "  score = Q @ K.T\n"
          ]
        }
      ],
      "source": [
        "embedding_dim = 512\n",
        "\n",
        "W = Tensor(encode_sentence(\"meu amor é maior que o seu e tudo que\".split()[:context_size], vocab)).reshape(context_size, 1)\n",
        "C = Tensor(embedding_dim).reshape(1, embedding_dim)\n",
        "P = Tensor(embedding_dim).reshape(1, embedding_dim)\n",
        "wQ = Tensor(embedding_dim, embedding_dim)\n",
        "wK = Tensor(embedding_dim, embedding_dim)\n",
        "wV = Tensor(embedding_dim, embedding_dim)\n",
        "w0 = Tensor(embedding_dim, embedding_dim)\n",
        "\n",
        "nn.init.xavier_uniform_(C)\n",
        "nn.init.xavier_uniform_(P)\n",
        "nn.init.xavier_uniform_(wQ)\n",
        "nn.init.xavier_uniform_(wK)\n",
        "nn.init.xavier_uniform_(wV)\n",
        "nn.init.xavier_uniform_(w0)\n",
        "\n",
        "def get_embeddings_with_attention(W, C, P, wQ, wK, wV, w0):\n",
        "\n",
        "    X = W @ C + P\n",
        "    Q = X @ wQ\n",
        "    K = X @ wK\n",
        "    V = X @ wV\n",
        "\n",
        "    scores = Q @ K.T\n",
        "    probs = F.softmax(scores, dim=-1)\n",
        "    E = probs @ V\n",
        "\n",
        "    return E @ w0\n",
        "\n",
        "def get_embeddings_with_attention_slow(W, C, P, wQ, wK, wV, w0):\n",
        "    E = []\n",
        "    X = W @ C + P\n",
        "\n",
        "    for xq in X:\n",
        "        Q = xq @ wQ\n",
        "        scores = []\n",
        "        for xk in X:\n",
        "            K = xk @ wK\n",
        "            score = Q @ K.T\n",
        "            scores.append(score)\n",
        "        scores = Tensor(scores)\n",
        "        probs = F.softmax(scores, dim = -1)\n",
        "\n",
        "        e = 0\n",
        "        for xv, p in zip(X, probs):\n",
        "            V = xv @ wV\n",
        "            e += V * p\n",
        "        e = e @ w0\n",
        "        E.append(e)\n",
        "\n",
        "    return torch.stack(E)\n",
        "\n",
        "A = get_embeddings_with_attention_slow(W, C, P, wQ, wK, wV, w0)\n",
        "B = get_embeddings_with_attention(W, C, P, wQ, wK, wV, w0)\n",
        "\n",
        "assert torch.allclose(A, B, atol=1e-5), \"Matrix and Loop implementations are not the same.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "# Implementation from Pytorch library - https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.Q = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.K = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.V = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.zero = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X = W @ C + P\n",
        "        Q = self.Q(X)\n",
        "        K = self.K(X)\n",
        "        V = self.V(X)\n",
        "\n",
        "        KT = K.permute(0, 2, 1)\n",
        "        scores = Q @ KT\n",
        "        probs = F.softmax(scores, dim=-1)\n",
        "        E = probs @ V\n",
        "\n",
        "        return self.zero(E)\n",
        "\n",
        "self_attention = SelfAttention(embedding_dim)\n",
        "X = W @ C + P\n",
        "X = X.reshape(1, X.shape[0], X.shape[1]) # Leave in batch form\n",
        "Y = self_attention(X)\n",
        "print(X.shape, Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.attention = SelfAttention(embedding_dim)\n",
        "        self.pe = PositionalEncoding(embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, vocab_size, bias = False)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs) # Lookup table\n",
        "        embeds = self.pe(embeds) # Positional Encoding\n",
        "        embeds = self.attention(embeds) # Self attention\n",
        "        embeds = embeds.view(embeds.size(0), -1) # Concat embeddings\n",
        "        out = torch.tanh(self.linear1(embeds)) # First layer with non linearity\n",
        "        out = self.relu(self.linear2(out)) # Second layer\n",
        "        log_probs = F.log_softmax(out, dim=1) # Logits\n",
        "        return log_probs\n",
        "\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 5]) torch.Size([16])\n",
            "torch.Size([16, 2500])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "output = model(input)\n",
        "print(input.shape, target.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training and Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 3060 Ti\n"
          ]
        }
      ],
      "source": [
        "# Verifica se há uma device disponível e define o dispositivo para device se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4861124"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# helper function to get accuracy from log probabilities\n",
        "def get_accuracy_from_log_probs(log_probs, labels):\n",
        "    probs = torch.exp(log_probs)\n",
        "    predicted_label = torch.argmax(probs, dim=1)\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "# helper function to evaluate model on dev data\n",
        "def evaluate(model, criterion, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_tensor, target_tensor in dataloader:\n",
        "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "            log_probs = model(context_tensor)\n",
        "            mean_loss += criterion(log_probs, target_tensor).item()\n",
        "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.00024598228628747165; Train Loss: 7.827923429250795, Train PPL: 2509.712158203125\n",
            "\n",
            "--- Training model Epoch: 1 ---\n",
            "Finished training of Epoch 1\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.13002213835716248; Train Loss: 5.844966642111393, Train PPL: 345.4909362792969\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.125; Test Loss: 5.91563205481513, Test PPL: 370.7886657714844\n",
            "\n",
            "--- Training model Epoch: 2 ---\n",
            "Finished training of Epoch 2\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.15271401405334473; Train Loss: 5.302257624404397, Train PPL: 200.78958129882812\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.14523263275623322; Test Loss: 5.409776848546799, Test PPL: 223.58164978027344\n",
            "\n",
            "--- Training model Epoch: 3 ---\n",
            "Finished training of Epoch 3\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.17231059074401855; Train Loss: 5.075045321565567, Train PPL: 159.9794158935547\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.15956750512123108; Test Loss: 5.2481020096870115, Test PPL: 190.2049560546875\n",
            "\n",
            "--- Training model Epoch: 4 ---\n",
            "Finished training of Epoch 4\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.1775992214679718; Train Loss: 4.895320506125601, Train PPL: 133.66282653808594\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.16661205887794495; Test Loss: 5.1440110487831685, Test PPL: 171.40188598632812\n",
            "\n",
            "--- Training model Epoch: 5 ---\n",
            "Finished training of Epoch 5\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.19145621359348297; Train Loss: 4.750664946289287, Train PPL: 115.66114044189453\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.1652195304632187; Test Loss: 5.069751603262765, Test PPL: 159.1348114013672\n",
            "\n",
            "--- Training model Epoch: 6 ---\n",
            "Finished training of Epoch 6\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.1969088315963745; Train Loss: 4.649657273378557, Train PPL: 104.54914855957031\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.16833224892616272; Test Loss: 5.045176522291191, Test PPL: 155.27169799804688\n",
            "\n",
            "--- Training model Epoch: 7 ---\n",
            "Finished training of Epoch 7\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.20621515810489655; Train Loss: 4.546386009437133, Train PPL: 94.29104614257812\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.17144495248794556; Test Loss: 5.009299406061635, Test PPL: 149.7997283935547\n",
            "\n",
            "--- Training model Epoch: 8 ---\n",
            "Finished training of Epoch 8\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.21611593663692474; Train Loss: 4.422182660526665, Train PPL: 83.27784729003906\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.17586828768253326; Test Loss: 4.959037185808934, Test PPL: 142.4565887451172\n",
            "\n",
            "--- Training model Epoch: 9 ---\n",
            "Finished training of Epoch 9\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.2260577231645584; Train Loss: 4.316883803743266, Train PPL: 74.9547119140625\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.17726081609725952; Test Loss: 4.939387490833727, Test PPL: 139.6846466064453\n",
            "\n",
            "--- Training model Epoch: 10 ---\n",
            "Finished training of Epoch 10\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.22954247891902924; Train Loss: 4.254405262197342, Train PPL: 70.4149398803711\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.1643184870481491; Test Loss: 4.963483675435517, Test PPL: 143.0914306640625\n",
            "BEST PPL: tensor(139.6846)\n"
          ]
        }
      ],
      "source": [
        "# Using negative log-likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "# create model\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, 500)\n",
        "\n",
        "# load it to gpu\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "\n",
        "train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "print(\"\\n--- Evaluating model on train data ---\")\n",
        "print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "\n",
        "best_test_ppl = 1e9\n",
        "for epoch in range(10):\n",
        "    st = time.time()\n",
        "    print(f\"\\n--- Training model Epoch: {epoch+1} ---\")\n",
        "    for it, data_tensor in enumerate(train_loader):       \n",
        "        context_tensor = data_tensor[0]\n",
        "        target_tensor = data_tensor[1]\n",
        "\n",
        "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "\n",
        "        # zero out the gradients from the old instance\n",
        "        model.zero_grad()\n",
        "        # get log probabilities over next words\n",
        "        log_probs = model(context_tensor)\n",
        "        # compute loss function\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "        # backward pass and update gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Finished training of Epoch {epoch +1}\\n--- Evaluating model on train data ---\")\n",
        "    train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "    print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "    print(\"\\n--- Evaluating model on test data ---\")\n",
        "    test_acc, test_loss = evaluate(model, loss_function, val_loader)\n",
        "    print(f\"Test Accuracy: {test_acc}; Test Loss: {test_loss}, Test PPL: {torch.exp(torch.tensor(test_loss))}\")\n",
        "\n",
        "    best_test_ppl = min(best_test_ppl, (torch.exp(torch.tensor(test_loss))))\n",
        "\n",
        "print(\"BEST PPL:\", best_test_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verdura e agrestes ; penetrar que o seu coração tinha a sua vida . \n",
            " -- e ! . . . \n",
            " o indio , a cabeça com um gesto de vozes e . \n",
            " o indio , a cabeça com um gesto de vozes e . \n",
            " o indio , a cabeça com um\n"
          ]
        }
      ],
      "source": [
        "i = 300\n",
        "text = \" \".join(final_tokens[i: i+context_size])\n",
        "\n",
        "inv_vocab = {v-1 : k for k, v in vocab.items()}\n",
        "def generate_text(model, vocab, text, max_length, context_size):\n",
        "    context = encode_sentence(text, vocab)\n",
        "\n",
        "    final_text = context\n",
        "    for i in range(max_length):\n",
        "        inputs = torch.tensor(context).to(device).view((1, -1))\n",
        "        pred = torch.argmax(model(inputs), dim=1)\n",
        "        final_text.append(pred.item())\n",
        "        context = final_text[-context_size:]\n",
        "\n",
        "    l = ([inv_vocab[t] for t in final_text])\n",
        "    decoded_sentence = \" \".join(l)\n",
        "\n",
        "    print(decoded_sentence)\n",
        "\n",
        "\n",
        "context = context_size\n",
        "max_length= 50\n",
        "generate_text(model, vocab, text, max_length, context_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
