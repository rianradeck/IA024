{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "## Exercício: Modelo de Linguagem com auto-atenção\n",
        "\n",
        "Este exercício é similar ao da aula passada, mas iremos agora treinar uma rede neural *com auto-atenção* para prever a próxima palavra de um texto, data as palavras anteriores como entrada.\n",
        "\n",
        "Na camada de auto-atenção, deve-se implementar (vide slide 34):\n",
        "- Embeddings de posição\n",
        "- Projeções lineares (WQ, WK, WV, WO)\n",
        "- Camada de feed forward (2-layer MLP)\n",
        "\n",
        "Instrucões:\n",
        "- É necessário fazer duas implementações da camada de auto-atenção: uma usando laços (ineficiente, mas fácil de entender) e outra matricial (eficiente mas difícil de entender). Usar slide 36 como referência.\n",
        "\n",
        "- Fazer um assert para garantir que o resultado das duas implementações é exatamente igual.\n",
        "\n",
        "- No treinamento, usar apenas a implementação matricial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n",
            "'wget' n�o � reconhecido como um comando interno\n",
            "ou externo, um programa oper�vel ou um arquivo em lotes.\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "!wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import multiprocessing\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4816"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simples limitação dos dados, para trabalhar apenas com tokens presentes no livro.\n",
        "\n",
        "text = open(\"67724.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text.find(\"PARTE\\n\\n\")\n",
        "idx2 = text.find(\"*** END OF THE PROJECT\")\n",
        "text = text[idx:idx2]\n",
        "text2 = open(\"67725.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text2.find(\"PARTE\\n\\n\")\n",
        "idx2 = text2.find(\"*** END OF THE PROJECT\")\n",
        "text2 = text2[idx:idx2]\n",
        "\n",
        "text += text2\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [],
      "source": [
        "# cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "cleaned_paragraphs = []\n",
        "full_text = \"\"\n",
        "final_tokens = []\n",
        "# Tratando tokens em cada prágrafo\n",
        "for paragraph in paragraphs:\n",
        "    paragraph = paragraph.replace(\"\\n\", \" \")\n",
        "    for removable in [\"«\", \"»\", \"_\"]:\n",
        "        paragraph = paragraph.replace(removable, '') # Removendo as aspas, underline, etc.\n",
        "    \n",
        "    paragraph = paragraph.lower().strip() # Caixa baixa e removendo leading e trailing spaces.\n",
        "\n",
        "    if paragraph[:3] == \"pag\":\n",
        "        continue\n",
        "    if len(paragraph) < 3:\n",
        "        continue\n",
        "\n",
        "    paragraph = re.sub(\"[ ]+\", \" \", paragraph) # Espaços duplicados\n",
        "\n",
        "    for punctuation in ['.', ',', ';', '!', \":\", \"?\", \"--\"]:\n",
        "        paragraph = paragraph.replace(punctuation, (' ' + punctuation) if punctuation != \"--\" else (punctuation + ' ')) # Tratando pontuação como próprio token\n",
        "    cleaned_paragraphs.append(paragraph)\n",
        "    final_tokens += paragraph.split(\" \") + ['\\n']\n",
        "    full_text += paragraph + '\\n'\n",
        "    \n",
        "# for paragraph in cleaned_paragraphs:\n",
        "#     print(paragraph)\n",
        "\n",
        "# final_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "127721"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        if text == \"\\n\":\n",
        "            word_counts.update(text)\n",
        "            continue\n",
        "        # word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "        word_counts.update(list(re.findall(r'.*', text.lower())))\n",
        "        \n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(final_tokens)\n",
        "word_counts.pop('')\n",
        "\n",
        "# word_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 2500\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1360,\n",
              " 2386,\n",
              " 50,\n",
              " 886,\n",
              " 1243,\n",
              " 1,\n",
              " 1536,\n",
              " 225,\n",
              " 0,\n",
              " 1,\n",
              " 11,\n",
              " 0,\n",
              " 7,\n",
              " 0,\n",
              " 1,\n",
              " 11,\n",
              " 1120,\n",
              " 879,\n",
              " 1,\n",
              " 0,\n",
              " 11,\n",
              " 103,\n",
              " 8,\n",
              " 1366,\n",
              " 14,\n",
              " 335,\n",
              " 1357,\n",
              " 86,\n",
              " 104,\n",
              " 4,\n",
              " 91,\n",
              " 12,\n",
              " 82,\n",
              " 35,\n",
              " 0,\n",
              " 26,\n",
              " 0,\n",
              " 593,\n",
              " 18,\n",
              " 14,\n",
              " 1362,\n",
              " 8,\n",
              " 580,\n",
              " 945,\n",
              " 2]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    if isinstance(sentence, str):\n",
        "        sentence = sentence.split(\" \")\n",
        "    # print(sentence)\n",
        "    return [vocab.get(word, 0) for word in sentence]\n",
        "\n",
        "encode_sentence(cleaned_paragraphs[20], vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[([1, 2, 35, 5, 591], 36), ([2, 35, 5, 591, 36], 1355), ([35, 5, 591, 36, 1355], 6), ([5, 591, 36, 1355, 6], 1356), ([591, 36, 1355, 6, 1356], 23)]\n"
          ]
        }
      ],
      "source": [
        "context_size = 5 # 5 palavras de entrada. O target é a próxima palavra\n",
        "\"\"\"TODO: Preparar o dataset\"\"\"\n",
        "overlap_size = 4\n",
        "step = context_size - overlap_size\n",
        "if step <= 0:\n",
        "    raise\n",
        "\n",
        "# print(final_tokens)\n",
        "whole_data = []\n",
        "for i in range(0, len(final_tokens) - context_size, step):\n",
        "    cur_data = [encode_sentence(final_tokens[i:i+context_size], vocab), encode_sentence(final_tokens[i + context_size], vocab)[0]]\n",
        "    if 0 in cur_data[0] or 0 == cur_data[1]:# or vocab_size in cur_data[0] or vocab_size == cur_data[1] :\n",
        "        continue\n",
        "    for i in range(context_size):\n",
        "        cur_data[0][i] -= 1\n",
        "    cur_data[1] -= 1\n",
        "    whole_data.append(tuple(cur_data))\n",
        "\n",
        "print(whole_data[:context_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "outputs": [],
      "source": [
        "N = len(whole_data)\n",
        "random_state = 18\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "random_indices = np.arange(N)\n",
        "np.random.shuffle(random_indices)\n",
        "# print(random_indices)\n",
        "cut_idx = int(0.8 * N)\n",
        "train_indices = random_indices[:cut_idx]\n",
        "validation_indices = random_indices[cut_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        idxs = train_indices if split == \"train\" else validation_indices\n",
        "        self.data = []\n",
        "        for idx in idxs:\n",
        "            self.data.append(whole_data[idx])\n",
        "            \n",
        "        self.vocab = vocab  # Set vocabulary\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Return the length of the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line, label = self.data[idx]  # Get label and text for specified index\n",
        "\n",
        "        return torch.tensor(line), torch.tensor(label)\n",
        "\n",
        "train_data = MyDataset(split=\"train\", vocab=vocab)\n",
        "val_data = MyDataset(split=\"val\", vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 174, 2494,    0,    7,    0],\n",
            "        [ 280,   66,    5,  244,    4],\n",
            "        [ 704,    5,   20,  389,   17],\n",
            "        [   2,    9,   22,   21,    2],\n",
            "        [   1,   33,  478,   14, 1760],\n",
            "        [  42, 2301,   29,  709,   11],\n",
            "        [   8,   37,    1,   45,  407],\n",
            "        [ 756,   11,  101,    1,    2],\n",
            "        [   3,  139,    7,    5,  735],\n",
            "        [ 914,   32,    1,    2,    5],\n",
            "        [ 458,   19,  147,  104,   63],\n",
            "        [   7,    5,   20,  128,    0],\n",
            "        [ 779,    0,    7,  382,  828],\n",
            "        [   0, 1903,  504,    0,  253],\n",
            "        [ 122,    3,  262,    1,    2],\n",
            "        [ 139,   12,  107,  726,    1],\n",
            "        [  35,  356,   21,    1,    1],\n",
            "        [1229,    4,  280,    8,   18],\n",
            "        [   2,    9,   66,   80,   50],\n",
            "        [  61,    0,    4,  280,   10],\n",
            "        [   0,   22,  761,    3, 1593],\n",
            "        [1458,  105,   24,  310,    0],\n",
            "        [   6,   13,  568,  152,    0],\n",
            "        [ 319,    1,    2,   12,   35],\n",
            "        [   0,    7,   46,    5,   20],\n",
            "        [   7,    3,   18,  197,    0],\n",
            "        [  55,  192,    8,    4,    3],\n",
            "        [  97,  332,   31,   10,  447],\n",
            "        [   7,   24, 1557,    6, 1133],\n",
            "        [   1,    1,   50,    5,  113]]), tensor([1037,   15,    3,    5,    0,  631,    1,   47,    1,   42,    0,    4,\n",
            "         639,  136,   19,    2,    1,  141,    4,  323,   18,   10,    7,  288,\n",
            "         126,    7,  108,  342,    6, 2116])]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 30\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "sample = next(iter(train_loader))\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, h):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, h)\n",
        "        self.linear2 = nn.Linear(h, h) # This hidden layer ideia I've got from Gabriel Freita's code. It helped to reduce PPL in 20.\n",
        "        self.linear3 = nn.Linear(h, vocab_size, bias = False)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs)\n",
        "        embeds = embeds.view(embeds.size(0), -1)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        out = self.relu(self.linear2(out))\n",
        "        out = self.linear3(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "embedding_dim = 128\n",
        "context_size = 5\n",
        "H = 500\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper function to get accuracy from log probabilities\n",
        "def get_accuracy_from_log_probs(log_probs, labels):\n",
        "    probs = torch.exp(log_probs)\n",
        "    predicted_label = torch.argmax(probs, dim=1)\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "# helper function to evaluate model on dev data\n",
        "def evaluate(model, criterion, dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        dev_st = time.time()\n",
        "        for it, data_tensor in enumerate(dataloader):\n",
        "            input = data_tensor[:,0:2]\n",
        "            target = data_tensor[:,2]\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            log_probs = model(input)\n",
        "            mean_loss += criterion(log_probs, target).item()\n",
        "            mean_acc += get_accuracy_from_log_probs(log_probs, target)\n",
        "            count += 1\n",
        "            if it % 500 == 0: \n",
        "                print(f\"Dev Iteration {it} complete. Mean Loss: {mean_loss / count}; Mean Acc: {mean_acc / count}; Time taken (s): {time.time()-dev_st}\")\n",
        "                dev_st = time.time()\n",
        "\n",
        "    return mean_acc / count, mean_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica se há uma device disponível e define o dispositivo para device se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30, 5]) torch.Size([30])\n",
            "torch.Size([30, 2500])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "print(input.shape, target.shape)\n",
        "output = model(input)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training and Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [],
      "source": [
        "# Verifica se há uma device disponível e define o dispositivo para device se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper function to get accuracy from log probabilities\n",
        "def get_accuracy_from_log_probs(log_probs, labels):\n",
        "    probs = torch.exp(log_probs)\n",
        "    predicted_label = torch.argmax(probs, dim=1)\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "# helper function to evaluate model on dev data\n",
        "def evaluate(model, criterion, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_tensor, target_tensor in dataloader:\n",
        "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "            log_probs = model(context_tensor)\n",
        "            mean_loss += criterion(log_probs, target_tensor).item()\n",
        "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.0006970071117393672; Train Loss: 7.81595775679322, Train PPL: 2479.8603515625\n",
            "\n",
            "--- Training model Epoch: 1 ---\n",
            "Finished training of Epoch 1\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.14135077595710754; Train Loss: 5.639600263457163, Train PPL: 281.3502197265625\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.1358957439661026; Test Loss: 5.726092808369629, Test PPL: 306.768310546875\n",
            "\n",
            "--- Training model Epoch: 2 ---\n",
            "Finished training of Epoch 2\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.16916663944721222; Train Loss: 5.296870964039735, Train PPL: 199.71096801757812\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.15735363960266113; Test Loss: 5.429516357051653, Test PPL: 228.03892517089844\n",
            "\n",
            "--- Training model Epoch: 3 ---\n",
            "Finished training of Epoch 3\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.1802302449941635; Train Loss: 5.1148640542012735, Train PPL: 166.47811889648438\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.16666682064533234; Test Loss: 5.293789130171042, Test PPL: 199.0963592529297\n",
            "\n",
            "--- Training model Epoch: 4 ---\n",
            "Finished training of Epoch 4\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.18650951981544495; Train Loss: 4.9798249585455485, Train PPL: 145.4489288330078\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.16922912001609802; Test Loss: 5.205532546711202, Test PPL: 182.27792358398438\n",
            "\n",
            "--- Training model Epoch: 5 ---\n",
            "Finished training of Epoch 5\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.19654549658298492; Train Loss: 4.855667103260646, Train PPL: 128.46636962890625\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.17380370199680328; Test Loss: 5.128642192929617, Test PPL: 168.7877655029297\n",
            "\n",
            "--- Training model Epoch: 6 ---\n",
            "Finished training of Epoch 6\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.204286590218544; Train Loss: 4.749651306376393, Train PPL: 115.54400634765625\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.17389723658561707; Test Loss: 5.071781671017921, Test PPL: 159.4581756591797\n",
            "\n",
            "--- Training model Epoch: 7 ---\n",
            "Finished training of Epoch 7\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.2127259522676468; Train Loss: 4.648561939395515, Train PPL: 104.4346923828125\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.1759798377752304; Test Loss: 5.022253524466287, Test PPL: 151.75289916992188\n",
            "\n",
            "--- Training model Epoch: 8 ---\n",
            "Finished training of Epoch 8\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.2154211699962616; Train Loss: 4.562957239180327, Train PPL: 95.86656951904297\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.1737217754125595; Test Loss: 4.987238720535354, Test PPL: 146.53128051757812\n",
            "\n",
            "--- Training model Epoch: 9 ---\n",
            "Finished training of Epoch 9\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.22833290696144104; Train Loss: 4.458778637215073, Train PPL: 86.3819580078125\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.18233293294906616; Test Loss: 4.9412109324621625, Test PPL: 139.9395751953125\n",
            "\n",
            "--- Training model Epoch: 10 ---\n",
            "Finished training of Epoch 10\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.23712743818759918; Train Loss: 4.371044611139051, Train PPL: 79.1262435913086\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.18595992028713226; Test Loss: 4.908424675611079, Test PPL: 135.42593383789062\n",
            "BEST PPL: tensor(135.4259)\n"
          ]
        }
      ],
      "source": [
        "# Using negative log-likelihood loss\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "# create model\n",
        "model = LanguageModel(len(vocab), embedding_dim, context_size, H)\n",
        "\n",
        "# load it to gpu\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "\n",
        "train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "print(\"\\n--- Evaluating model on train data ---\")\n",
        "print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "\n",
        "best_test_ppl = 1e9\n",
        "for epoch in range(10):\n",
        "    st = time.time()\n",
        "    print(f\"\\n--- Training model Epoch: {epoch+1} ---\")\n",
        "    for it, data_tensor in enumerate(train_loader):       \n",
        "        context_tensor = data_tensor[0]\n",
        "        target_tensor = data_tensor[1]\n",
        "\n",
        "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "\n",
        "        # zero out the gradients from the old instance\n",
        "        model.zero_grad()\n",
        "        # get log probabilities over next words\n",
        "        log_probs = model(context_tensor)\n",
        "        # compute loss function\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "        # backward pass and update gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Finished training of Epoch {epoch +1}\\n--- Evaluating model on train data ---\")\n",
        "    train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "    print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "    print(\"\\n--- Evaluating model on test data ---\")\n",
        "    test_acc, test_loss = evaluate(model, loss_function, val_loader)\n",
        "    print(f\"Test Accuracy: {test_acc}; Test Loss: {test_loss}, Test PPL: {torch.exp(torch.tensor(test_loss))}\")\n",
        "\n",
        "    best_test_ppl = min(best_test_ppl, (torch.exp(torch.tensor(test_loss))))\n",
        "\n",
        "print(\"BEST PPL:\", best_test_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "frente \n",
            " a que della se o seu olhar de sua senhora , e a menina , que se tinha a sua vida , e que não se o que lhe se não era o que se tinha a sua vida , e que não se o que lhe se não era o que se tinha a sua vida , e que não se o que lhe se não era o que se tinha a sua vida , e que não se o que lhe se não era o que se tinha a sua vida , e que não se o que lhe se não\n"
          ]
        }
      ],
      "source": [
        "i = 1000\n",
        "text = \" \".join(final_tokens[i: i+context_size])\n",
        "\n",
        "inv_vocab = {v-1 : k for k, v in vocab.items()}\n",
        "def generate_text(model, vocab, text, max_length, context_size):\n",
        "    context = encode_sentence(text, vocab)\n",
        "\n",
        "    final_text = context\n",
        "    for i in range(max_length):\n",
        "        inputs = torch.tensor(context).to(device).view((1, -1))\n",
        "        pred = torch.argmax(model(inputs), dim=1)\n",
        "        final_text.append(pred.item())\n",
        "        context = final_text[-context_size:]\n",
        "\n",
        "    l = ([inv_vocab[t] for t in final_text])\n",
        "    decoded_sentence = \" \".join(l)\n",
        "\n",
        "    print(decoded_sentence)\n",
        "\n",
        "\n",
        "context = context_size\n",
        "max_length= 100\n",
        "generate_text(model, vocab, text, max_length, context_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
