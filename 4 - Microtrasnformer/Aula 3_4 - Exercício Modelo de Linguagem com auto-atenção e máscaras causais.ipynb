{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMI0JT_YuYF3"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Isto está formatado como código\n",
        "```\n",
        "\n",
        "## Exercício: Modelo de Linguagem com auto-atenção e máscaras causais\n",
        "\n",
        "Seguimos na mesma linha de treinar um modelo de linguagem a partir dos textos do livro \"O Guarani\", de José de Alencar.\n",
        "\n",
        "Neste exercício, vamos treinar um modelo de linguagem com auto-atenção e com máscara causal. A máscara causal é necessária para que o modelo não tenha acesso a palavras futuras, que é a abordagem usada por grandes modelos de linguagem, como o GPT.\n",
        "\n",
        "Use a implementação matricial de auto-atenção da aula passada.\n",
        "\n",
        "### Modificações necessárias\n",
        "\n",
        "* Adicione a máscara causal na função `forward` da cabeça de auto-atenção.\n",
        "* Modifique o nosso dataloader para retornar inputs (uma lista de tokens de tamanho $n$), targets (uma lista de tokens de tamanho $n$ deslocada para a esquerda em 1 token). Exemplo `input = [1, 2, 3, 4]`, `target = [2, 3, 4, 5]` para a sequência `[1, 2, 3, 4, 5]` com `seq_len=4`, por exemplo (Ver slide 50).\n",
        "\n",
        "### Extra\n",
        "* MultiHeadAttention: modifique a cabeça de auto-atenção para ter múltiplas cabeças. Isso não é obrigatório, mas pode ser interessante para ver como o modelo se comporta.\n",
        "* Diagrama da geração: fazer diagrama que mostre os passos da geração de tokens (conforme slide 47).\n",
        "\n",
        "### Dicas\n",
        "\n",
        "* Use como base o vídeo do Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. Observe que, no vídeo, ele primeiro implementa um modelo bi-grama, depois um modelo de linguagem com auto-atenção. O modelo de auto-atenção é implementado por volta do minuto 40, mas vale a pena assistir o vídeo todo.\n",
        "* Use esta implementação como base: https://colab.research.google.com/drive/1vFTg4MSXVJwNSzPjaCcvmqhxTP7gK7HA?usp=sharing. Observe como o modelo é organizado e como a máscara é implementada na classe MultiHeadAttention.\n",
        "* Use `context_size=9`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYbkEzdD37sZ"
      },
      "source": [
        "## Faz download e carrega o dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qAnqY_q0beK",
        "outputId": "f810fdb0-138d-4917-b7ef-69ab266acef6"
      },
      "outputs": [],
      "source": [
        "# !wget https://www.gutenberg.org/ebooks/67724.txt.utf-8\n",
        "# !wget https://www.gutenberg.org/ebooks/67725.txt.utf-8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import multiprocessing\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torch import nn, Tensor\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 307,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_UzC9pV091C",
        "outputId": "1553b04f-24c4-4027-8cab-0907f92f04df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4816"
            ]
          },
          "execution_count": 307,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Simples limitação dos dados, para trabalhar apenas com tokens presentes no livro.\n",
        "\n",
        "text = open(\"67724.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text.find(\"PARTE\\n\\n\")\n",
        "idx2 = text.find(\"*** END OF THE PROJECT\")\n",
        "text = text[idx:idx2]\n",
        "text2 = open(\"67725.txt.utf-8\",\"r\",encoding=\"utf-8\").read()\n",
        "idx = text2.find(\"PARTE\\n\\n\")\n",
        "idx2 = text2.find(\"*** END OF THE PROJECT\")\n",
        "text2 = text2[idx:idx2]\n",
        "\n",
        "text += text2\n",
        "\n",
        "paragraphs = text.split(\"\\n\\n\")\n",
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 308,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhUFjtNdDuG0",
        "outputId": "78798c0c-deca-4454-d3fb-7d3ba70f3e91"
      },
      "outputs": [],
      "source": [
        "# cleaned_paragraphs = [paragraph.replace(\"\\n\", \" \") for paragraph in paragraphs if paragraph.strip()]\n",
        "\n",
        "cleaned_paragraphs = []\n",
        "full_text = \"\"\n",
        "final_tokens = []\n",
        "# Tratando tokens em cada prágrafo\n",
        "for paragraph in paragraphs:\n",
        "    paragraph = paragraph.replace(\"\\n\", \" \")\n",
        "    for removable in [\"«\", \"»\", \"_\"]:\n",
        "        paragraph = paragraph.replace(removable, '') # Removendo as aspas, underline, etc.\n",
        "    \n",
        "    paragraph = paragraph.lower().strip() # Caixa baixa e removendo leading e trailing spaces.\n",
        "\n",
        "    if paragraph[:3] == \"pag\":\n",
        "        continue\n",
        "    if len(paragraph) < 3:\n",
        "        continue\n",
        "\n",
        "    paragraph = re.sub(\"[ ]+\", \" \", paragraph) # Espaços duplicados\n",
        "\n",
        "    for punctuation in ['.', ',', ';', '!', \":\", \"?\", \"--\"]:\n",
        "        paragraph = paragraph.replace(punctuation, (' ' + punctuation) if punctuation != \"--\" else (punctuation + ' ')) # Tratando pontuação como próprio token\n",
        "    cleaned_paragraphs.append(paragraph)\n",
        "    final_tokens += paragraph.split(\" \") + ['\\n']\n",
        "    full_text += paragraph + '\\n'\n",
        "    \n",
        "# for paragraph in cleaned_paragraphs:\n",
        "#     print(paragraph)\n",
        "\n",
        "# final_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 309,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSRHqe3H4ZFw",
        "outputId": "4a985c7a-ce1d-4b72-d253-c9fbbc5f9440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "127721"
            ]
          },
          "execution_count": 309,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Conta as palavras no dataset\n",
        "\n",
        "def count_words(texts):\n",
        "    word_counts = Counter()\n",
        "    for text in texts:\n",
        "        if text == \"\\n\":\n",
        "            word_counts.update(text)\n",
        "            continue\n",
        "        # word_counts.update(re.findall(r'\\w+', text.lower()))\n",
        "        word_counts.update(list(re.findall(r'.*', text.lower())))\n",
        "        \n",
        "    return word_counts\n",
        "\n",
        "word_counts = count_words(final_tokens)\n",
        "word_counts.pop('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyGVDL9KzJ_I"
      },
      "source": [
        "## Criando um vocabulário"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "id": "FiP7OCo9zJ_I"
      },
      "outputs": [],
      "source": [
        "vocab_size = 2500\n",
        "most_frequent_words = [word for word, count in word_counts.most_common(vocab_size)]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhbhAsZbzJ_J",
        "outputId": "6a53c9e0-308d-4082-e225-cfa376e8f39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1360, 2386, 50, 886, 1243, 1, 1536, 225, 0, 1, 11, 0, 7, 0, 1, 11, 1120, 879, 1, 0, 11, 103, 8, 1366, 14, 335, 1357, 86, 104, 4, 91, 12, 82, 35, 0, 26, 0, 593, 18, 14, 1362, 8, 580, 945, 2]\n"
          ]
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    if isinstance(sentence, str):\n",
        "        sentence = sentence.split(\" \")\n",
        "    # print(sentence)\n",
        "    return [vocab.get(word, 0) for word in sentence]\n",
        "\n",
        "print(encode_sentence(cleaned_paragraphs[20], vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wia_ygbvzJ_J"
      },
      "source": [
        "## Classe do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "Iy-elI1magRR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[([1, 2, 35, 5, 591, 36, 1355, 6, 1356], [2, 35, 5, 591, 36, 1355, 6, 1356, 23]), ([2, 35, 5, 591, 36, 1355, 6, 1356, 23], [35, 5, 591, 36, 1355, 6, 1356, 23, 1356]), ([35, 5, 591, 36, 1355, 6, 1356, 23, 1356], [5, 591, 36, 1355, 6, 1356, 23, 1356, 0]), ([5, 591, 36, 1355, 6, 1356, 23, 1356, 0], [591, 36, 1355, 6, 1356, 23, 1356, 0, 2378]), ([591, 36, 1355, 6, 1356, 23, 1356, 0, 2378], [36, 1355, 6, 1356, 23, 1356, 0, 2378, 28]), ([36, 1355, 6, 1356, 23, 1356, 0, 2378, 28], [1355, 6, 1356, 23, 1356, 0, 2378, 28, 13]), ([1355, 6, 1356, 23, 1356, 0, 2378, 28, 13], [6, 1356, 23, 1356, 0, 2378, 28, 13, 1357]), ([6, 1356, 23, 1356, 0, 2378, 28, 13, 1357], [1356, 23, 1356, 0, 2378, 28, 13, 1357, 0]), ([1356, 23, 1356, 0, 2378, 28, 13, 1357, 0], [23, 1356, 0, 2378, 28, 13, 1357, 0, 226])]\n"
          ]
        }
      ],
      "source": [
        "context_size = 9\n",
        "\n",
        "# print(final_tokens)\n",
        "whole_data = []\n",
        "for i in range(0, len(final_tokens) - context_size, 1):\n",
        "    cur_data = [encode_sentence(final_tokens[i:i+context_size], vocab), encode_sentence(final_tokens[i+1:i+1 + context_size], vocab)]\n",
        "    if 0 in cur_data[0] or 0 in cur_data[1]:\n",
        "        continue\n",
        "    for i in range(context_size):\n",
        "        cur_data[0][i] -= 1\n",
        "        cur_data[1][i] -= 1\n",
        "    \n",
        "    whole_data.append(tuple(cur_data))\n",
        "\n",
        "print(whole_data[:context_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "y1aetOpmDeQd"
      },
      "outputs": [],
      "source": [
        "N = len(whole_data)\n",
        "random_state = 18\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "random_indices = np.arange(N)\n",
        "np.random.shuffle(random_indices)\n",
        "# print(random_indices)\n",
        "cut_idx = int(0.8 * N)\n",
        "train_indices = random_indices[:cut_idx]\n",
        "validation_indices = random_indices[cut_idx:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD1CVci2zJ_J",
        "outputId": "5bf0839e-f30e-4ff2-ed6f-4f3fda782b7c"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        idxs = train_indices if split == \"train\" else validation_indices\n",
        "        self.data = []\n",
        "        for idx in idxs:\n",
        "            self.data.append(whole_data[idx])\n",
        "            \n",
        "        self.vocab = vocab  # Set vocabulary\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Return the length of the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line, label = self.data[idx]  # Get label and text for specified index\n",
        "\n",
        "        return torch.tensor(line), torch.tensor(label)\n",
        "\n",
        "train_data = MyDataset(split=\"train\", vocab=vocab)\n",
        "val_data = MyDataset(split=\"val\", vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "id": "gC0C5qn2zJ_J"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "sample = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_dim = 512\n",
        "\n",
        "W = Tensor(encode_sentence(\"meu amor é maior que o seu e tudo que\".split()[:context_size], vocab)).reshape(context_size, 1)\n",
        "C = Tensor(embedding_dim).reshape(1, embedding_dim)\n",
        "P = Tensor(embedding_dim).reshape(1, embedding_dim)\n",
        "wQ = Tensor(embedding_dim, embedding_dim)\n",
        "wK = Tensor(embedding_dim, embedding_dim)\n",
        "wV = Tensor(embedding_dim, embedding_dim)\n",
        "w0 = Tensor(embedding_dim, embedding_dim)\n",
        "\n",
        "nn.init.xavier_uniform_(C)\n",
        "nn.init.xavier_uniform_(P)\n",
        "nn.init.xavier_uniform_(wQ)\n",
        "nn.init.xavier_uniform_(wK)\n",
        "nn.init.xavier_uniform_(wV)\n",
        "nn.init.xavier_uniform_(w0)\n",
        "\n",
        "def get_embeddings_with_attention(W, C, P, wQ, wK, wV, w0):\n",
        "\n",
        "    X = W @ C + P\n",
        "    Q = X @ wQ\n",
        "    K = X @ wK\n",
        "    V = X @ wV\n",
        "\n",
        "    scores = Q @ K.T\n",
        "    probs = F.softmax(scores, dim=-1)\n",
        "    E = probs @ V\n",
        "\n",
        "    return E @ w0\n",
        "\n",
        "def get_embeddings_with_attention_slow(W, C, P, wQ, wK, wV, w0):\n",
        "    E = []\n",
        "    X = W @ C + P\n",
        "\n",
        "    for xq in X:\n",
        "        Q = xq @ wQ\n",
        "        scores = []\n",
        "        for xk in X:\n",
        "            K = xk @ wK\n",
        "            score = Q @ K.T\n",
        "            scores.append(score)\n",
        "        scores = Tensor(scores)\n",
        "        probs = F.softmax(scores, dim = -1)\n",
        "\n",
        "        e = 0\n",
        "        for xv, p in zip(X, probs):\n",
        "            V = xv @ wV\n",
        "            e += V * p\n",
        "        e = e @ w0\n",
        "        E.append(e)\n",
        "\n",
        "    return torch.stack(E)\n",
        "\n",
        "A = get_embeddings_with_attention_slow(W, C, P, wQ, wK, wV, w0)\n",
        "B = get_embeddings_with_attention(W, C, P, wQ, wK, wV, w0)\n",
        "\n",
        "assert torch.allclose(A, B, atol=1e-4), \"Matrix and Loop implementations are not the same.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_-Yud0zJ_K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "metadata": {
        "id": "I2qKG9YczJ_K"
      },
      "outputs": [],
      "source": [
        "# Implementation from Pytorch library - https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 9, 512]) torch.Size([1, 9, 512])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.Q = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.K = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.V = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.zero = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X = W @ C + P\n",
        "        Q = self.Q(X)\n",
        "        K = self.K(X)\n",
        "        V = self.V(X)\n",
        "\n",
        "        KT = K.permute(0, 2, 1)\n",
        "        scores = Q @ KT\n",
        "\n",
        "        # Causal bias\n",
        "        mask = torch.ones(scores.shape[0], context_size, context_size, dtype=torch.bool)\n",
        "        mask = mask.triu(diagonal=1)\n",
        "        scores[mask.reshape(scores.shape[0], context_size, context_size)] = -torch.inf\n",
        "\n",
        "        probs = F.softmax(scores, dim=-1)\n",
        "        E = probs @ V\n",
        "\n",
        "        return self.zero(E)\n",
        "\n",
        "self_attention = SelfAttention(embedding_dim)\n",
        "X = W @ C + P\n",
        "X = X.reshape(1, X.shape[0], X.shape[1]) # Leave in batch form\n",
        "Y = self_attention(X)\n",
        "print(X.shape, Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        # n_embd: embedding dimension\n",
        "        super().__init__()\n",
        "        self.sa = SelfAttention(n_embd)\n",
        "        self.ffwd = FeedFoward(n_embd, 0.0)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_blocks = 5\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, n_blocks):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pe = PositionalEncoding(embedding_dim)\n",
        "        \n",
        "        self.blocks = nn.Sequential(*[Block(embedding_dim) for _ in range(n_blocks)])\n",
        "        \n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs) # Lookup table\n",
        "        embeds = self.pe(embeds) # Positional Encoding\n",
        "        \n",
        "        out = self.blocks(embeds)\n",
        "\n",
        "        out = self.linear(out)\n",
        "        log_probs = F.log_softmax(out, dim=1) # Logits\n",
        "        return log_probs\n",
        "\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, n_blocks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "metadata": {
        "id": "xmsD59TfzJ_K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 9]) torch.Size([16, 9])\n",
            "torch.Size([16, 9, 2500])\n"
          ]
        }
      ],
      "source": [
        "sample = next(iter(train_loader))\n",
        "input = sample[0]\n",
        "target = sample[1]\n",
        "output = model(input)\n",
        "print(input.shape, target.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UngUhyu7zJ_L"
      },
      "source": [
        "## Training and Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wntaV50nzJ_L",
        "outputId": "a054092b-d801-4c60-eb75-85abfe57151d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA GeForce RTX 3060 Ti\n"
          ]
        }
      ],
      "source": [
        "# Verifica se há uma device disponível e define o dispositivo para device se possível, caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "18321920"
            ]
          },
          "execution_count": 324,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# helper function to get accuracy from log probabilities\n",
        "def get_accuracy_from_log_probs(log_probs, labels):\n",
        "    probs = torch.exp(log_probs)\n",
        "    predicted_label = torch.argmax(probs, dim=1)\n",
        "    acc = (predicted_label == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "# helper function to evaluate model on dev data\n",
        "def evaluate(model, criterion, dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    mean_acc, mean_loss = 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for context_tensor, target_tensor in dataloader:\n",
        "            context_tensor, target_tensor = context_tensor.to(device), target_tensor.reshape(-1).to(device)\n",
        "            \n",
        "            log_probs = model(context_tensor).view(-1, vocab_size)\n",
        "            mean_loss += criterion(log_probs, target_tensor).item()\n",
        "            mean_acc += get_accuracy_from_log_probs(log_probs, target_tensor)\n",
        "            count += 1\n",
        "\n",
        "    return mean_acc / count, mean_loss / count\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "count_parameters(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "id": "vRwSPiwizJ_L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.0002399808872723952; Train Loss: 8.014495424523982, Train PPL: 3024.484130859375\n",
            "\n",
            "--- Training model Epoch: 1 ---\n",
            "Finished training of Epoch 1\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.22328327596187592; Train Loss: 6.7589684560026, Train PPL: 861.752685546875\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.21853268146514893; Test Loss: 6.809593844362263, Test PPL: 906.5023803710938\n",
            "\n",
            "--- Training model Epoch: 2 ---\n",
            "Finished training of Epoch 2\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.2536183297634125; Train Loss: 6.453123595750615, Train PPL: 634.6817016601562\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.24364055693149567; Test Loss: 6.534398586652191, Test PPL: 688.4196166992188\n",
            "\n",
            "--- Training model Epoch: 3 ---\n",
            "Finished training of Epoch 3\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.3127700090408325; Train Loss: 4.647805727970008, Train PPL: 104.35574340820312\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.30392059683799744; Test Loss: 4.7659839708398275, Test PPL: 117.4466323852539\n",
            "\n",
            "--- Training model Epoch: 4 ---\n",
            "Finished training of Epoch 4\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.3718319237232208; Train Loss: 4.075013370673548, Train PPL: 58.851253509521484\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.35980620980262756; Test Loss: 4.231830942450538, Test PPL: 68.84317016601562\n",
            "\n",
            "--- Training model Epoch: 5 ---\n",
            "Finished training of Epoch 5\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.4089270532131195; Train Loss: 3.739768947174946, Train PPL: 42.08826446533203\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.3907187879085541; Test Loss: 3.944858638050747, Test PPL: 51.669029235839844\n",
            "\n",
            "--- Training model Epoch: 6 ---\n",
            "Finished training of Epoch 6\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.440582275390625; Train Loss: 3.4627482313059317, Train PPL: 31.904539108276367\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.41911134123802185; Test Loss: 3.7098452756523312, Test PPL: 40.84748840332031\n",
            "\n",
            "--- Training model Epoch: 7 ---\n",
            "Finished training of Epoch 7\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.46102967858314514; Train Loss: 3.2994335220390467, Train PPL: 27.09728240966797\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.4357301592826843; Test Loss: 3.572743577524601, Test PPL: 35.614173889160156\n",
            "\n",
            "--- Training model Epoch: 8 ---\n",
            "Finished training of Epoch 8\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.48858538269996643; Train Loss: 3.1193632748677973, Train PPL: 22.63196563720703\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.45662355422973633; Test Loss: 3.422993361821185, Test PPL: 30.66105842590332\n",
            "\n",
            "--- Training model Epoch: 9 ---\n",
            "Finished training of Epoch 9\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.5041698813438416; Train Loss: 2.985623704563207, Train PPL: 19.798845291137695\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.46533796191215515; Test Loss: 3.3286373527724593, Test PPL: 27.900297164916992\n",
            "\n",
            "--- Training model Epoch: 10 ---\n",
            "Finished training of Epoch 10\n",
            "--- Evaluating model on train data ---\n",
            "Train Accuracy: 0.5115039944648743; Train Loss: 2.8954685338115076, Train PPL: 18.09197425842285\n",
            "\n",
            "--- Evaluating model on test data ---\n",
            "Test Accuracy: 0.47070738673210144; Test Loss: 3.248952380009398, Test PPL: 25.763336181640625\n",
            "BEST PPL: tensor(25.7633)\n"
          ]
        }
      ],
      "source": [
        "# Using negative log-likelihood loss\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# create model\n",
        "model = LanguageModel(vocab_size, embedding_dim, context_size, n_blocks)\n",
        "\n",
        "# load it to gpu\n",
        "model = model.to(device)\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
        "\n",
        "train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "print(\"\\n--- Evaluating model on train data ---\")\n",
        "print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "\n",
        "# loss = compute_loss(model, train_loader, loss_function)\n",
        "\n",
        "best_test_ppl = 1e9\n",
        "for epoch in range(10):\n",
        "    st = time.time()\n",
        "    print(f\"\\n--- Training model Epoch: {epoch+1} ---\")\n",
        "    for it, data_tensor in enumerate(train_loader):       \n",
        "        context_tensor = data_tensor[0]\n",
        "        target_tensor = data_tensor[1].reshape(-1)\n",
        "\n",
        "        context_tensor, target_tensor = context_tensor.to(device), target_tensor.to(device)\n",
        "\n",
        "        # zero out the gradients from the old instance\n",
        "        model.zero_grad()\n",
        "        # get log probabilities over next words\n",
        "        log_probs = model(context_tensor)\n",
        "        log_probs = log_probs.view(-1, vocab_size)\n",
        "        # compute loss function\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "        # backward pass and update gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Finished training of Epoch {epoch +1}\\n--- Evaluating model on train data ---\")\n",
        "    train_acc, train_loss = evaluate(model, loss_function, train_loader)\n",
        "    print(f\"Train Accuracy: {train_acc}; Train Loss: {train_loss}, Train PPL: {torch.exp(torch.tensor(train_loss))}\")\n",
        "    print(\"\\n--- Evaluating model on test data ---\")\n",
        "    test_acc, test_loss = evaluate(model, loss_function, val_loader)\n",
        "    print(f\"Test Accuracy: {test_acc}; Test Loss: {test_loss}, Test PPL: {torch.exp(torch.tensor(test_loss))}\")\n",
        "\n",
        "    best_test_ppl = min(best_test_ppl, (torch.exp(torch.tensor(test_loss))))\n",
        "\n",
        "print(\"BEST PPL:\", best_test_ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1zhxVqfzJ_M"
      },
      "source": [
        "## Exemplo de uso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "id": "3PExkoWOzJ_M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verdura e agrestes ; penetrar luxo \n",
            " a que não tinha um . \n",
            " um com um tinha um \n",
            " -- um senhora ! tinha um ; um . um um cada tinha uma chamma um pery . \n",
            " -- , tinha um pery . \n",
            " -- um ! tinha um pery ! . um chegava um para não\n"
          ]
        }
      ],
      "source": [
        "i = 300\n",
        "text = \" \".join(final_tokens[i: i+context_size])\n",
        "# text = \" \".join(\"gosto bastante de carne mas sempre como frango no jantar\".split()[:context_size])\n",
        "\n",
        "inv_vocab = {v-1 : k for k, v in vocab.items()}\n",
        "def generate_text(model, vocab, text, max_length, context_size):\n",
        "    context = encode_sentence(text, vocab)\n",
        "\n",
        "    final_text = context\n",
        "    for i in range(max_length):\n",
        "        inputs = torch.tensor(context).to(device).view((1, -1))\n",
        "        # print(inputs.shape, model(inputs).shape)\n",
        "        pred = torch.argmax(model(inputs)[0][-1])\n",
        "        final_text.append(pred.item())\n",
        "        context = final_text[-context_size:]\n",
        "\n",
        "    l = ([inv_vocab[t] for t in final_text])\n",
        "    decoded_sentence = \" \".join(l)\n",
        "\n",
        "    print(decoded_sentence)\n",
        "\n",
        "\n",
        "context = context_size\n",
        "max_length= 50\n",
        "generate_text(model, vocab, text, max_length, context_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
