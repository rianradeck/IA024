{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# THIS WAS RAN IN COLAB, NOT LOCALY AS USUAL\n",
        "https://colab.research.google.com/drive/1Y1eJ7NgCZbFUsFpn4IcMCAWQCk8heXLo?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhjrdWXuPMeW"
      },
      "source": [
        "## Module 2 - Fine-tuning Phi-1.5 for sentence classification using QLoRA\n",
        "\n",
        "This notebook presents an example of how to fine-tune Phi-1.5 for sentence classification using QLoRA.\n",
        "\n",
        "QLoRA is a fine-tuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. For more details, please refer to the [QLoRA paper](https://arxiv.org/abs/2106.09647).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dFZ6y7XMXD4"
      },
      "source": [
        "# Installing required packages\n",
        "\n",
        "In this example, we have to install the following libraries:  `transformers`, `datasets`, `torch`, `peft`, `bitsandbytes`, and `trl`.\n",
        "\n",
        "**`transformers`**:\n",
        "\n",
        "Transformers is an open-source library for NLP developed by Hugging Face. It provides state-of-the-art pre-trained models for various NLP tasks, such as text classification, sentiment analysis, question-answering, named entity recognition, etc.\n",
        "\n",
        "**`datasets`**:\n",
        "\n",
        "Datasets is another open-source library developed by Hugging Face that provides a collection of preprocessed datasets for various NLP tasks, such as sentiment analysis, natural language inference, machine translation, and many more.\n",
        "\n",
        "\n",
        "**`torch`**:\n",
        "\n",
        "PyTorch is an open-source machine learning library that provides a wide range of tools and utilities for building and training custom deep learning models. It is already installed in the Colab environment, but we need to install its latest version.\n",
        "\n",
        "**`peft`**:\n",
        "\n",
        "ðŸ¤— PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the modelâ€™s parameters. We use PEFT in this example because it supports QLoRA.\n",
        "\n",
        "\n",
        "**`bitsandbytes`**:\n",
        "\n",
        "BitsAndBytes is a library designed to optimize the training of neural networks on modern GPUs. It offers efficient implementations of 8-bit optimizers, which significantly reduce the memory footprint of model parameters and gradients. This reduction in memory usage enables training larger models or using larger batch sizes within the same memory constraints.\n",
        "\n",
        "\n",
        "**`trl`**:\n",
        "\n",
        "ðŸ¤— TRL, or Transfer Learning Library, is a library for training and evaluating transfer learning models. It provides a unified API for training and evaluating various transfer learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdrBm7o4LPRb",
        "outputId": "2b305b68-8a9b-449a-a850-8492eb1f133e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch\n",
        "!pip install -q git+https://github.com/huggingface/transformers #huggingface transformers for downloading models weights\n",
        "!pip install datasets\n",
        "!pip install -q peft  # Parameter efficient finetuning - for qLora Finetuning\n",
        "!pip install -q bitsandbytes  # For Model weights quantization\n",
        "!pip install -q trl  # Transformer Reinforcement Learning - For Finetuning using Supervised Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGdm9g9fkjxE"
      },
      "source": [
        "# Setting the device\n",
        "\n",
        "In this example, we will use a GPU to speed up the fine-tuning process. GPUs (Graphics Processing Units) are specialized processors that are optimized for performing large-scale computations in parallel. By using a GPU, we can accelerate the training and inference of a machine learning model, which can significantly reduce the time required to complete these tasks.\n",
        "\n",
        "Before we begin, we need to check whether a GPU is available and select it as the default device for our PyTorch operations. This is because PyTorch can use either a CPU or a GPU to perform computations, and by default, it will use the CPU.\n",
        "\n",
        "For using a GPU in Google Colab:\n",
        "1. Click on the \"Runtime\" menu at the top of the screen.\n",
        "2. From the dropdown menu, click on \"Change runtime type\".\n",
        "3. In the popup window that appears, select \"A100 GPU\" as the hardware accelerator.\n",
        "4. Click on the \"Save\" button.\n",
        "\n",
        "That's it! Now you can use the GPU for faster computations in your notebook.\n",
        "\n",
        "**IMPORTANT**: This example requires a GPU with at least 40GB of memory. If you are using Google Colab, you can select a GPU with 40GB of memory by following the steps above. If you are using a different environment, please make sure that your GPU has at least 40GB of memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiV9Y1ZweR2Y",
        "outputId": "ecb592c0-38fa-43ff-c858-7fe9bbaa56d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Apr 25 04:36:13 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwYGaELZMwvZ"
      },
      "source": [
        "# Downloading Dataset\n",
        "\n",
        "The SST-2 dataset, or the Stanford Sentiment Treebank, is popular for sentiment analysis tasks in Natural Language Processing (NLP). It consists of movie reviews from the Rotten Tomatoes website that are labeled with either a positive or negative sentiment. The dataset contains 10,662 sentence-level movie reviews, with approximately half of the reviews labeled as positive and the other half labeled as negative. The reviews are also relatively evenly distributed in length, with a median length of 18 tokens.\n",
        "\n",
        "The SST-2 dataset has become a benchmark dataset for sentiment analysis in NLP, and many researchers use it to evaluate the performance of their models. The dataset's popularity is partly due to its high-quality labels and the task's relative simplicity, making it an accessible starting point for researchers and developers new to NLP.\n",
        "\n",
        "In this example, we're using the **`datasets`** library to download and load the training and validation sets of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbX4laO5M6FC",
        "outputId": "60850434-ee3b-483c-e632-92fb1104894d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset('stanfordnlp/imdb', split='train')\n",
        "test_dataset = load_dataset('stanfordnlp/imdb', split='test')\n",
        "\n",
        "train_dataset  = train_dataset.shuffle(seed=42)\n",
        "test_dataset  = test_dataset.shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zrj5qInTBEP"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(lambda example: {'text': example['text'].replace(\"<br />\", \" \")})\n",
        "test_dataset = test_dataset.map(lambda example: {'text': example['text'].replace(\"<br />\", \" \")})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OyZ3D_rKEK2"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "Now, we will prepare the data for training our model. First, we define a template with the fields `sentence` and `class`. Then, we use the `map` method to apply this template to the dataset. This will create a new dataset with the fields `sentence` and `class` for each example in the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBjlqME6VUJn"
      },
      "outputs": [],
      "source": [
        "# Thanks to Pedro Rodrigues\n",
        "template = \"\"\"Your task is to classify sentences' sentiment as 'positive' or 'negative'. Your answer MUST be ONLY one word, either 'positive' or 'negative'. Sentence:{text}. Answer:{class}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1ZalJmCxLn9"
      },
      "source": [
        "Before, we need to convert the labels from 0 and 1 to \"negative\" and \"positive\". We can do this by using the `map` method to apply a function to each example in the dataset. The function will take the label as input and return the corresponding string and store in the column `class`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65ew6vQDKEz7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "POSITIVE_LABEL = \"positive\"\n",
        "NEGATIVE_LABEL = \"negative\"\n",
        "\n",
        "train_dataset = train_dataset.map(lambda example: {\"class\": \"{}\".format(POSITIVE_LABEL if example[\"label\"] == 1 else NEGATIVE_LABEL)})\n",
        "train_dataset = train_dataset.map(lambda example: {\"text\": template.format(**example)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE2EQjQ69uLJ"
      },
      "source": [
        "The code below converts the `label` column of the test dataset into a list of strings with `\"positive\"` and `\"negative\"` labels. This is for comparing the model's predictions with the actual labels of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCts-_zF9uLM",
        "outputId": "ace4da7e-0898-49e4-cefc-984f10285ee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': \"Your task is to classify sentences' sentiment as 'positive' or 'negative'. Your answer MUST be ONLY one word, either 'positive' or 'negative'. Sentence:  When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?  Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!  I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly played. I also recognized myself and this movie was an eye-opener, a relief, a chance to face my OWN truth and finally doing something about it. I truly hope A Thousand Acres has had the same effect on some others out there.  Since I didn't understand why the cover said the film was about sisters fighting over land -they weren't fighting each other at all- I watched it a second time. Then I was able to see that if one hadn't lived a similar story, one would easily miss the overwhelming undercurrent of dread and fear and the deep bond between the sisters that runs through it all. That is exactly the reason why people in general often overlook the truth about their neighbors for instance.  But yet another reason why this movie is so perfect!  I don't give a rat's ass (pardon my French) about to what extend the King Lear story is followed. All I know is that I can honestly say: this movie has changed my life.  Keep up the good work guys, you CAN and DO make a difference.  . Answer:positive\",\n",
              " 'label': 1,\n",
              " 'class': 'positive'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset = test_dataset.map(lambda example: {\"class\": \"{}\".format(POSITIVE_LABEL if example[\"label\"] == 1 else NEGATIVE_LABEL)})\n",
        "test_dataset = test_dataset.map(lambda example: {\"text\": template.format(**example)})\n",
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyAho1DLxg7z"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-_4YGOYRpmu"
      },
      "source": [
        "## Setting Model Parameters\n",
        "\n",
        "We need to set various parameters for our fine-tuning process, including QLoRA (Quantization LoRA) parameters, bitsandbytes parameters, and training arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H76_erzt1-on"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "# model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "model_name = \"microsoft/phi-1_5\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"phi-1_5-IMDB\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKzTy9MbR2DY"
      },
      "source": [
        "Setting the QLora Parameters\n",
        "\n",
        "1. **lora_r (LoRA attention dimension)**:\n",
        "   - the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
        "\n",
        "2. **lora_alpha (Alpha parameter for LoRA scaling)**:\n",
        "   - This parameter is the LoRA scaling factor applied to the modifications.\n",
        "\n",
        "3. **lora_dropout (Dropout probability for LoRA layers)**:\n",
        "   - This parameter represents the dropout rate applied to the LoRA layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXH06fiYybTF"
      },
      "outputs": [],
      "source": [
        "# LoRA attention dimension\n",
        "lora_r = 64 # @param\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16 # @param\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1 # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g69uRG5jzCEZ"
      },
      "source": [
        "Bitsandbytes parameters. These parameters focus on the implementation of 4-bit precision in model loading and computation. Here's an explanation of each:\n",
        "\n",
        "1. **use_4bit (Activate 4-bit precision base model loading)**:\n",
        "   - This parameter, when set to `True`, indicates that the base model (i.e., the pre-trained model or initial model weights) should be loaded using 4-bit precision.\n",
        "2. **bnb_4bit_compute_dtype (Compute dtype for 4-bit base models)**:\n",
        "   - This parameter specifies the data type to be used for computations in the context of 4-bit base models.\n",
        "   - The value `\"float16\"` indicates that computations should be done using 16-bit floating-point numbers.\n",
        "\n",
        "3. **bnb_4bit_quant_type (Quantization type)**:\n",
        "   - This parameter determines the type of quantization to be used for the 4-bit models.\n",
        "   - The options `\"fp4\"` and `\"nf4\"` refer to different quantization schemes.\n",
        "\n",
        "4. **use_nested_quant (Activate nested quantization for 4-bit base models)**:\n",
        "   - When set to `True`, this parameter enables nested quantization for 4-bit base models.\n",
        "   - Nested quantization, often referred to as double quantization, involves applying a second layer of quantization on top of an already quantized model. This can be used for further reducing the model size or for specialized computational optimizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtraXCCMzJAc"
      },
      "outputs": [],
      "source": [
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True # @param\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\" # @param\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\" # @param [\"nf4\",\"fp4\"]\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHTSrnU00F2u"
      },
      "source": [
        "Now, let's define the training arguments.\n",
        "\n",
        "1. **output_dir**:\n",
        "   - Specifies the directory where the model predictions and checkpoints will be stored.\n",
        "\n",
        "2. **num_train_epochs**:\n",
        "   - Sets the number of epochs for training, where one epoch means one pass through the entire training dataset. We set it to `1`\n",
        "\n",
        "3. **fp16, bf16**:\n",
        "   - Enable training with 16-bit floating-point precision (`fp16`) or 16-bit bfloat precision (`bf16`).\n",
        "\n",
        "4. **per_device_train_batch_size**:\n",
        "   - Determines the batch size for training per GPU. This will depend on the GPU used. For an A100, we can use a batch size of 16 examples.\n",
        "\n",
        "5. **per_device_eval_batch_size**:\n",
        "   - Sets the batch size for evaluation per GPU.\n",
        "\n",
        "6. **gradient_accumulation_steps**:\n",
        "   - Indicates the number of update steps over which to accumulate gradients.\n",
        "\n",
        "7. **gradient_checkpointing**:\n",
        "   - When enabled, saves memory by trading compute for memory. Useful for training large models that would otherwise not fit in memory.\n",
        "\n",
        "8. **max_grad_norm (Maximum gradient norm)**:\n",
        "   - Specifies the maximum norm of gradients for gradient clipping, a technique to prevent exploding gradients in deep networks.\n",
        "\n",
        "9. **learning_rate**:\n",
        "   - Sets the initial learning rate for the AdamW optimizer.\n",
        "\n",
        "10. **weight_decay**:\n",
        "    - Specifies the weight decay to apply to all layers except those with bias or LayerNorm weights, as a regularization technique.\n",
        "\n",
        "11. **optim**:\n",
        "    - Defines the optimizer to use, here specified as a variant of AdamW optimized for certain hardware configurations.\n",
        "\n",
        "12. **lr_scheduler_type**:\n",
        "    - Determines the learning rate schedule to use. \"constant\" means the learning rate stays the same throughout training.\n",
        "\n",
        "13. **max_steps**:\n",
        "    - Overrides `num_train_epochs` by setting the number of training steps. If set to a negative value, it's ignored. We set this to `100` to reduce the training time. That means, that our example training does not use the entire traing set.\n",
        "\n",
        "14. **warmup_ratio**:\n",
        "    - Indicates the proportion of total training steps to use for linear warmup of the learning rate.\n",
        "\n",
        "15. **group_by_length**:\n",
        "    - When enabled, sequences are grouped by length into batches. This can save memory and speed up training.\n",
        "\n",
        "16. **save_steps**:\n",
        "    - Determines how often to save a model checkpoint in terms of training steps.\n",
        "\n",
        "17. **logging_steps**:\n",
        "    - Sets the frequency, in terms of training steps, for logging training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayWlSa-s0SJ2"
      },
      "outputs": [],
      "source": [
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\" # @param\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1 # @param\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False # @param\n",
        "bf16 = False # @param\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 4 # @param\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 4 # @param\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1 # @param\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True # @param\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3 # @param\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 5e-4 # @param\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001 # @param\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\" # @param\n",
        "\n",
        "# Learning rate schedule (constant a bit better than cosine)\n",
        "lr_scheduler_type = \"constant\" # @param\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = 1000 # @param\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03 # @param\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True # @param\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 25 # @param\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25 # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ypfndp911mbp"
      },
      "source": [
        "Now let's defint the SFTTrainer parameters\n",
        "\n",
        "1. **max_seq_length**:\n",
        "   - This parameter specifies the maximum sequence length to be used.\n",
        "\n",
        "2. **packing**:\n",
        "   - This parameter indicates whether or not to pack multiple short examples into the same input sequence.\n",
        "   - When set to `True`, this technique can be used to increase computational efficiency, particularly in batch processing.\n",
        "\n",
        "3. **device_map**:\n",
        "   - This parameter is a dictionary that maps parts of the model to specific computing devices.\n",
        "   - The entry `{\"\": 0}` specifies that the entire model will be loaded onto GPU 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_Yi1p_BR0bA"
      },
      "outputs": [],
      "source": [
        "# Maximum sequence length to use\n",
        "max_seq_length = 512\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269-A6vtSNfg"
      },
      "source": [
        "### Lets Load the base model\n",
        "Let's load the Mistral 7B Instruct base model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rg--UCDDZ43f"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pprint import pprint\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, PeftModel\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from trl import SFTTrainer # For supervised finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd00Qz6P2LDn"
      },
      "source": [
        "Load the base model with QLoRA configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrBd2aB95aoj",
        "outputId": "3fb08729-647e-4f14-d70a-5a96c1f87ca4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PhiConfig {\n",
              "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
              "  \"architectures\": [\n",
              "    \"PhiForCausalLM\"\n",
              "  ],\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"auto_map\": {\n",
              "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_phi.PhiConfig\",\n",
              "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_phi.PhiForCausalLM\"\n",
              "  },\n",
              "  \"bos_token_id\": null,\n",
              "  \"embd_pdrop\": 0.0,\n",
              "  \"eos_token_id\": null,\n",
              "  \"hidden_act\": \"gelu_new\",\n",
              "  \"hidden_size\": 2048,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 8192,\n",
              "  \"layer_norm_eps\": 1e-05,\n",
              "  \"max_position_embeddings\": 2048,\n",
              "  \"model_type\": \"phi\",\n",
              "  \"num_attention_heads\": 32,\n",
              "  \"num_hidden_layers\": 24,\n",
              "  \"num_key_value_heads\": 32,\n",
              "  \"partial_rotary_factor\": 0.5,\n",
              "  \"pretraining_tp\": 1,\n",
              "  \"qk_layernorm\": false,\n",
              "  \"quantization_config\": {\n",
              "    \"_load_in_4bit\": true,\n",
              "    \"_load_in_8bit\": false,\n",
              "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
              "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
              "    \"bnb_4bit_quant_type\": \"nf4\",\n",
              "    \"bnb_4bit_use_double_quant\": false,\n",
              "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
              "    \"llm_int8_has_fp16_weight\": false,\n",
              "    \"llm_int8_skip_modules\": null,\n",
              "    \"llm_int8_threshold\": 6.0,\n",
              "    \"load_in_4bit\": true,\n",
              "    \"load_in_8bit\": false,\n",
              "    \"quant_method\": \"bitsandbytes\"\n",
              "  },\n",
              "  \"resid_pdrop\": 0.0,\n",
              "  \"rope_scaling\": null,\n",
              "  \"rope_theta\": 10000.0,\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"torch_dtype\": \"float16\",\n",
              "  \"transformers_version\": \"4.41.0.dev0\",\n",
              "  \"use_cache\": false,\n",
              "  \"vocab_size\": 51200\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "base_model.config.use_cache = False\n",
        "# base_model.config.rope_scaling = None\n",
        "base_model.config.pretraining_tp = 1\n",
        "\n",
        "base_model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_zPMcIO2M4S"
      },
      "source": [
        "Load tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5STGmYHw2ORV"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxYdQFqo5aol"
      },
      "source": [
        "## Fine-Tuning with QLoRA and Supervised Fine-Tuning\n",
        "\n",
        "We're ready to fine-tune our model using QLoRA. For this tutorial, we'll use the `SFTTrainer` from the `trl` library.\n",
        "\n",
        "In the context of the code below, `target_modules` refers to specific components or layers of a neural network model that will be modified or adapted using LoRA (Low-Rank Adaptation). LoRA is a technique used to adapt pre-trained models with minimal additional parameters, often used in the context of Transformer models. Here's a breakdown of what each module likely represents:\n",
        "\n",
        "1. **q_proj, k_proj, v_proj, o_proj**:\n",
        "   - These refer to the projections for query (q), key (k), value (v), and output (o) in the attention mechanism of a Transformer model.\n",
        "\n",
        "2. **gate_proj**:\n",
        "   - This refer to a projection layer associated with gating mechanisms in the model, such as those found in Gated Recurrent Units (GRUs) or similar structures.\n",
        "\n",
        "3. **up_proj, down_proj**:\n",
        "   - These refer to projection layers used in upsampling or downsampling within the model.\n",
        "\n",
        "4. **lm_head**:\n",
        "   - This refers to the language model head of a Transformer, which is the final layer that produces the output (like the next word in a sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "60b1d024b49d4fd4b001c3b414ccf1f3",
            "213e91afbd1f475e98720c50dbdf9eff",
            "ac7e36c6f410462c83d5b46ae582c4dd",
            "0474271d26b94c48aace2b743dde364b",
            "64a5c831f2d842eea62025e558dc97b7",
            "49b07ee0a5cd49778644b532adb16d9e",
            "ab31f4579032428a815e0cb7e2a6125b",
            "08f381afd35f4fe2be49f08fa08af217",
            "42cf3b1e26814af8ba76817cf309331f",
            "28c5c3b3d0e54571bc20e4a7cd3e3e9a",
            "1584d145ef2d4436b98b76f1aace20c4"
          ]
        },
        "id": "nFjA8D0d2oNz",
        "outputId": "5abdd110-6ece-4845-ec56-73b9c3c81c8d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60b1d024b49d4fd4b001c3b414ccf1f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMcVmQqlQJPQ"
      },
      "source": [
        "## Evaluate untrained, unmerged model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKZDwNrrQN1f",
        "outputId": "06875e5f-ec47-437c-afd2-07d79a1b1944"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [07:51<00:00,  2.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers:[('', 914), ('Sent', 86)]\n",
            "Memory Used: 0\n",
            "Acc: 0.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'Sent',\n",
              " '']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def string_accuracy(predictions, references):\n",
        "    correct = sum([1 for p, r in zip(predictions, references) if r.lower()[:3] in p.lower()])\n",
        "    total = len(predictions)\n",
        "    return correct / total\n",
        "\n",
        "def get_memory():\n",
        "    memory_info = !nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits\n",
        "    return int(memory_info[0])\n",
        "\n",
        "device = \"cuda:0\"\n",
        "prompt = \"\"\"Is this sentence positive or negative? Answer only with \"positive\" or \"negative\".\n",
        "Sentence: {sentence}\n",
        "Answer: \"\"\"\n",
        "def classify_sentence(sentence, model):\n",
        "  text = prompt.format(sentence=sentence)\n",
        "  encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  model_inputs = encodeds.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.generate(**model_inputs,max_new_tokens=2,bos_token_id=model.config.bos_token_id,\n",
        "                                eos_token_id=model.config.eos_token_id,\n",
        "                                pad_token_id=model.config.eos_token_id\n",
        "                             )\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "#   print(tokenizer.decode(outputs[0][len(model_inputs[\"input_ids\"][0]):], skip_special_tokens=True))\n",
        "  return tokenizer.decode(outputs[0][len(model_inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
        "\n",
        "classify_sentence(\"This movie is too good\", base_model)\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "def evaluate(model, dataset):\n",
        "\n",
        "    answers = []\n",
        "    # print(dataset)\n",
        "    references  = dataset[\"class\"]\n",
        "\n",
        "    start_memory = get_memory()\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        model_answer = classify_sentence(item['text'], model)\n",
        "        answers.append(model_answer.strip())\n",
        "\n",
        "    final_memory = get_memory() - start_memory\n",
        "    acc = string_accuracy(answers, references)\n",
        "\n",
        "    print(f\"Answers:{Counter(answers).most_common()}\\nMemory Used: {final_memory}\\nAcc: {acc}\")\n",
        "\n",
        "    return answers\n",
        "\n",
        "# print(test_dataset)\n",
        "evaluate(base_model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx39lD5TQEvV"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i187wmtT5aol"
      },
      "source": [
        "## Let's start the training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xWxi9tqgEhyZ",
        "outputId": "aeec9e59-fa9d-408f-d4df-a7b53e97e71a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 45:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.289400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.814100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.879700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>3.109100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>3.121500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.675000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>3.143000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.838000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>3.156000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.808400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>3.109600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.771300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>3.098300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.720200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>3.140900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.699100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>3.117400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.743700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>3.149400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.831500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>3.139900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.756300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>3.059200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.756200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>3.120100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.662900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>3.146200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.739800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>3.133000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.709100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>3.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>3.102300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.681600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>3.124100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.676200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>3.124400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.761700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Save trained model\n",
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogpQjxbnWv3l"
      },
      "source": [
        "## Evaluate trained, unmerged model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zUob75UiWta6",
        "outputId": "79fe908f-d8d4-4309-ebfa-58c5345ca97b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [07:55<00:00,  2.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers:[('positive.', 582), ('???? Answer', 315), ('positive', 78), ('**********', 13), ('***********', 4), ('********positive', 3), ('positive Answer', 2), ('*********', 2), ('******** National', 1)]\n",
            "Memory Used: -12928\n",
            "Acc: 0.378\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '**********',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '**********',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '**********',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '********positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '**********',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '***********',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '********positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '*********',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '**********',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '**********',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '**********',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '**********',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '**********',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '**********',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '**********',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " '**********',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '**********',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '***********',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '***********',\n",
              " '*********',\n",
              " '******** National',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '***********',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '********positive',\n",
              " '???? Answer',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " '???? Answer',\n",
              " 'positive.',\n",
              " 'positive.',\n",
              " 'positive.']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "evaluate(base_model, test_dataset)\n",
        "# classify_sentence(\"This movie is too bad\", base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PixsfVRz38YA"
      },
      "source": [
        "## Merge the fine-tuned model\n",
        "\n",
        "After fine-tuning, we can merge the fine-tuned model with the base model to get a single model that can be used for inference. This is done by using the PEFT. First, let's clean up the GPU memory by deleting the fine-tuned model. You can also restart the runtime to clear the GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ujf17z-7--l5",
        "outputId": "3870ff17-eebc-427e-b95b-b32e668fa3a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "import gc\n",
        "del base_model\n",
        "gc.collect()\n",
        "\n",
        "del trainer\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ypNIoSou-_3j"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z-cOpLlC_A48",
        "outputId": "480cdb51-13f9-4bc1-cd6f-bf302a008ae8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20148"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAYoqNyp7m7n"
      },
      "source": [
        "Now, let's load the base model and fine-tuned model and merge them using PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IRWhV5WZ3-BO"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0},\n",
        ")\n",
        "merged_model= PeftModel.from_pretrained(base_model, new_model,)\n",
        "merged_model= merged_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NlvU92-7xnR"
      },
      "source": [
        "Let's save our merged model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oIYyv_y07zrZ"
      },
      "outputs": [],
      "source": [
        "# Save the merged model\n",
        "merged_model.save_pretrained(\"merged_model\", safe_serialization=True)\n",
        "tokenizer.save_pretrained(\"merged_model\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_hU2be-5aom"
      },
      "source": [
        "## Test the merged model\n",
        "\n",
        "The following code performs the inference stage of the evaluation finetuned Mistral-7B-Instruct model. We define a function called **`classify_sentence`** that is designed to use a pretrained model, likely a variant of a large language model similar to GPT, for sentiment analysis. The description below outlines the steps taken in the function to classify the sentiment of a given sentence as either positive, negative, or possibly neutral. I'll expand on the description by going through the function step-by-step:\n",
        "\n",
        "1. The function accepts a single parameter, `sentence`, which is the text input whose sentiment is to be classified.\n",
        "\n",
        "2. The `sentence` is formatted with the predefined prompt template. This prompt engineering is a common practice when using language models for specific tasks, as it provides context to the model about the task it is supposed to perform.\n",
        "\n",
        "3. The `tokenizer` is applied to the formatted text. Tokenizers convert text into a format that models can understand, which in this case is a series of tokens. The tokenizer is configured to:\n",
        "   - Return tensors compatible with PyTorch (`return_tensors=\"pt\"`).\n",
        "   - Not add special tokens that are usually used to indicate the start and end of a sequence (`add_special_tokens=False`).\n",
        "\n",
        "4. The tokenized input (`encodeds`) is then converted to a PyTorch tensor and moved to the appropriate device (GPU) for inference.\n",
        "\n",
        "5. The inference is performed inside a `torch.no_grad()` context manager, which disables gradient calculations. This is used because we are making predictions, not training the model, and therefore do not need gradients, which would only use extra memory and computational power.\n",
        "\n",
        "6. The `model.generate` function is called to generate a response. This function takes several parameters, such as:\n",
        "   - `**model_inputs`: The tokenized inputs prepared earlier.\n",
        "   - `max_length=8000`: This sets the maximum length of the model's output. The choice of 8000 seems unusually high for sentence classification and might be tailored to specific requirements of the task or the model's capacity.\n",
        "   - `bos_token_id=model.config.bos_token_id`: This specifies the beginning-of-sentence token id, signaling the model where a new sentence starts.\n",
        "   - `eos_token_id=model.config.eos_token_id`: This specifies the end-of-sentence token id, signaling the model where a sentence ends.\n",
        "   - `pad_token_id=model.config.eos_token_id`: This is used for padding shorter sentences to a uniform length. It's unusual to see the end-of-sentence token used for padding, which could be a specific requirement of this model or a mistake.\n",
        "\n",
        "7. After the model generates a response, `torch.cuda.empty_cache()` is called to free up unused memory on the GPU. This is helpful in managing GPU resources, especially when processing multiple requests or dealing with large models.\n",
        "\n",
        "8. Finally, the `tokenizer.decode` function is used to convert the model's output tokens back into human-readable text. The `skip_special_tokens=True` argument removes any special tokens (like padding or end-of-sentence tokens) from the output. The function also skips the input tokens (`outputs[0][len(model_inputs[\"input_ids\"][0]):]`) to only return the newly generated text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O28cCRIP4ZsJ"
      },
      "source": [
        "The code below uses the **`classify_sentence`** function to make predictions on the test dataset. We loop through the test dataset and apply the **`classify_sentence`** function to each example. The predictions are stored in a list called **`predictions`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XXEyUMHwVKJ-",
        "outputId": "73f3329f-0ee4-42ee-f8ca-7e2c392a3607"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positive.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classify_sentence(\"This movie is too good\", merged_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyKJR-CmVwxF"
      },
      "source": [
        "# Evaluation Metric\n",
        "\n",
        "To compute accuracy, we need to define a custom **`string_accuracy`** function since model outputs text rather than numerical values. Therefore, we cannot use the built-in accuracy function directly, which expects numerical values as inputs.\n",
        "\n",
        "The following code defines the **`string_accuracy`** function. It takes two lists of strings as inputs, **`predictions`** and **`references`**. The function computes accuracy by counting the number of predictions that match the corresponding reference and dividing by the total number of predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9SltQDZS9zyt",
        "outputId": "31867409-b428-4511-f8b9-ea177e59dafd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:35<00:00, 10.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answers:[('positive.', 659), ('???? Answer', 230), ('**********', 35), ('positive', 31), ('********positive', 20), ('***********', 18), ('*********', 3), ('Answer', 3), ('******** National', 1)]\n",
            "Memory Used: -2\n",
            "Acc: 0.4\n"
          ]
        }
      ],
      "source": [
        "references  = test_dataset[\"class\"]\n",
        "predictions = evaluate(merged_model, test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO0ibw-L-Y8t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# def get_binary\n",
        "\n",
        "# Convert labels to a numerical form\n",
        "labels = {\"positive\": 1, \"negative\": 0}\n",
        "y_true_num = [labels[label] for label in references]\n",
        "y_pred_num = [labels.get(label, -1) for label in predictions]\n",
        "print(y_true_num)\n",
        "print(y_pred_num)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true_num, y_pred_num)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Other\", \"Negative\", \"Positive\"], yticklabels=[\"Other\", \"Negative\", \"Positive\"])\n",
        "\n",
        "# Labels, title, and ticks\n",
        "ax.set_ylabel('Actual Label')\n",
        "ax.set_xlabel('Predicted Label')\n",
        "ax.set_title('Confusion Matrix')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0474271d26b94c48aace2b743dde364b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c5c3b3d0e54571bc20e4a7cd3e3e9a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1584d145ef2d4436b98b76f1aace20c4",
            "value": "â€‡25000/25000â€‡[00:21&lt;00:00,â€‡1250.60â€‡examples/s]"
          }
        },
        "08f381afd35f4fe2be49f08fa08af217": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1584d145ef2d4436b98b76f1aace20c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "213e91afbd1f475e98720c50dbdf9eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b07ee0a5cd49778644b532adb16d9e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ab31f4579032428a815e0cb7e2a6125b",
            "value": "Map:â€‡100%"
          }
        },
        "28c5c3b3d0e54571bc20e4a7cd3e3e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42cf3b1e26814af8ba76817cf309331f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49b07ee0a5cd49778644b532adb16d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b1d024b49d4fd4b001c3b414ccf1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_213e91afbd1f475e98720c50dbdf9eff",
              "IPY_MODEL_ac7e36c6f410462c83d5b46ae582c4dd",
              "IPY_MODEL_0474271d26b94c48aace2b743dde364b"
            ],
            "layout": "IPY_MODEL_64a5c831f2d842eea62025e558dc97b7"
          }
        },
        "64a5c831f2d842eea62025e558dc97b7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab31f4579032428a815e0cb7e2a6125b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac7e36c6f410462c83d5b46ae582c4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08f381afd35f4fe2be49f08fa08af217",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42cf3b1e26814af8ba76817cf309331f",
            "value": 25000
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
